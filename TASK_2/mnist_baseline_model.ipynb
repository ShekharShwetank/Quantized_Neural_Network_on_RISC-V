{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f001ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Shape of x_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f331a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train after reshaping: (60000, 28, 28, 1)\n",
      "Shape of x_test after reshaping: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Reshape images\n",
    "# Images are 28x28 grayscale, so we add a channel dimension of 1\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Shape of x_train after reshaping: {x_train.shape}\")\n",
    "print(f\"Shape of x_test after reshaping: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75feeec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min pixel value in x_train after normalization: 0.0\n",
      "Max pixel value in x_train after normalization: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Normalize pixel values\n",
    "# Convert pixel values from [0, 255] to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(f\"Min pixel value in x_train after normalization: {x_train.min()}\")\n",
    "print(f\"Max pixel value in x_train after normalization: {x_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6af7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train after one-hot encoding: (60000, 10)\n",
      "Shape of y_test after one-hot encoding: (10000, 10)\n",
      "Example of one-hot encoded label (first training label): [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: One-hot encode labels\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Shape of y_train after one-hot encoding: {y_train.shape}\")\n",
    "print(f\"Shape of y_test after one-hot encoding: {y_test.shape}\")\n",
    "print(f\"Example of one-hot encoded label (first training label): {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94261c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101770 (397.54 KB)\n",
      "Trainable params: 101770 (397.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Network Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)), # Input layer: Flattens 28x28x1 images to a 784-element vector\n",
    "    Dense(128, activation='relu'),   # Hidden layer: 128 neurons with ReLU activation\n",
    "    Dense(10, activation='softmax')  # Output layer: 10 neurons (for 10 classes) with Softmax activation\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16e78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\n"
     ]
    }
   ],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b88313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.3503 - accuracy: 0.9037 - val_loss: 0.2015 - val_accuracy: 0.9420\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1669 - accuracy: 0.9519 - val_loss: 0.1377 - val_accuracy: 0.9601\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.9648 - val_loss: 0.1117 - val_accuracy: 0.9672\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0945 - accuracy: 0.9726 - val_loss: 0.0969 - val_accuracy: 0.9704\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.9775 - val_loss: 0.0914 - val_accuracy: 0.9720\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0640 - accuracy: 0.9814 - val_loss: 0.0894 - val_accuracy: 0.9721\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.9850 - val_loss: 0.0758 - val_accuracy: 0.9765\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 0.0764 - val_accuracy: 0.9776\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9901 - val_loss: 0.0746 - val_accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0319 - accuracy: 0.9912 - val_loss: 0.0736 - val_accuracy: 0.9770\n",
      "\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,        # Number of times to iterate over the entire training dataset\n",
    "    batch_size=128,   # Number of samples per gradient update\n",
    "    validation_data=(x_test, y_test) # Data to evaluate the loss and metrics on at the end of each epoch\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc97f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n",
      "Test Loss: 0.0736\n",
      "Test Accuracy: 0.9770\n",
      "\n",
      "Making predictions on a few test images...\n",
      "1/1 [==============================] - 0s 84ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference test completed. Predicted vs. True labels for 5 samples displayed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict a few test images\n",
    "print(\"\\nMaking predictions on a few test images...\")\n",
    "predictions = model.predict(x_test[:5]) # Predict on the first 5 test images\n",
    "\n",
    "# Display images and predictions\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {np.argmax(y_test[i])}\\nPred: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInference test completed. Predicted vs. True labels for 5 samples displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08091bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model File Size: 1247056 bytes (1217.83 KB, 1.19 MB)\n",
      "Total Model Parameters: 101770\n",
      "Estimated Memory for Weights (float32): 407080 bytes (397.54 KB, 0.39 MB)\n",
      "Estimated Peak Activation Memory for single inference: 6824 bytes (6.66 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint\n",
    "# Save the model to a temporary file to get its size on disk\n",
    "model_filename = \"mnist_baseline_model.h5\"\n",
    "model.save(model_filename)\n",
    "model_size_bytes = os.path.getsize(model_filename)\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "model_size_mb = model_size_kb / 1024\n",
    "\n",
    "print(f\"\\nModel File Size: {model_size_bytes} bytes ({model_size_kb:.2f} KB, {model_size_mb:.2f} MB)\")\n",
    "\n",
    "# To remove the temporary file after checking its size\n",
    "os.remove(model_filename)\n",
    "\n",
    "# Estimate memory for inference (weights and activations)\n",
    "# Parameters (weights and biases) are already calculated in model.summary()\n",
    "total_params = model.count_params()\n",
    "# Assuming float32 (4 bytes per parameter)\n",
    "weights_memory_bytes = total_params * 4\n",
    "weights_memory_kb = weights_memory_bytes / 1024\n",
    "weights_memory_mb = weights_memory_kb / 1024\n",
    "\n",
    "print(f\"Total Model Parameters: {total_params}\")\n",
    "print(f\"Estimated Memory for Weights (float32): {weights_memory_bytes} bytes ({weights_memory_kb:.2f} KB, {weights_memory_mb:.2f} MB)\")\n",
    "\n",
    "# Estimating activation memory for a single inference:\n",
    "# Input layer: 28*28*1 * 4 bytes/pixel (float32)\n",
    "# Flatten layer output: 784 * 4 bytes\n",
    "# Dense layer 1 output: 128 * 4 bytes\n",
    "# Dense layer 2 output: 10 * 4 bytes\n",
    "input_mem = 28 * 28 * 1 * 4\n",
    "flatten_output_mem = 784 * 4\n",
    "dense1_output_mem = 128 * 4\n",
    "dense2_output_mem = 10 * 4\n",
    "total_activation_mem_single_inference = input_mem + flatten_output_mem + dense1_output_mem + dense2_output_mem\n",
    "\n",
    "print(f\"Estimated Peak Activation Memory for single inference: {total_activation_mem_single_inference} bytes ({total_activation_mem_single_inference / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15769287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model File Size: 1247056 bytes (1217.83 KB, 1.19 MB)\n",
      "\n",
      "Keras model saved to mnist_baseline_model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpfexi95xv\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpfexi95xv\\assets\n",
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized TFLite model saved to mnist_quantized_model.tflite\n",
      "Quantized TFLite Model Size: 104496 bytes (102.05 KB)\n",
      "Cleaned up temporary Keras model file: mnist_baseline_model.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint\n",
    "# Save the model to a temporary file to get its size on disk\n",
    "model_filename = \"mnist_baseline_model.h5\"\n",
    "model.save(model_filename)\n",
    "model_size_bytes = os.path.getsize(model_filename)\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "model_size_mb = model_size_kb / 1024\n",
    "\n",
    "print(f\"\\nModel File Size: {model_size_bytes} bytes ({model_size_kb:.2f} KB, {model_size_mb:.2f} MB)\")\n",
    "\n",
    "# To remove the temporary file after checking its size\n",
    "os.remove(model_filename)\n",
    "\n",
    "# Convert the Keras model to a TensorFlow Lite model with full integer quantization\n",
    "\n",
    "# Re-save the model in the native Keras format (recommended by TensorFlow warning)\n",
    "keras_model_path = \"mnist_baseline_model.keras\"\n",
    "model.save(keras_model_path)\n",
    "print(f\"\\nKeras model saved to {keras_model_path}\")\n",
    "\n",
    "# Load the model from the native Keras format for conversion\n",
    "loaded_model = keras.models.load_model(keras_model_path)\n",
    "\n",
    "# Create a TensorFlow Lite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "\n",
    "# Enable full integer quantization\n",
    "# This will quantize weights and activations to int8.\n",
    "# It requires a representative dataset for calibration to determine min/max ranges for activations.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Provide a representative dataset for calibration\n",
    "# The representative dataset should consist of a small subset of your training data.\n",
    "# TFLite uses this to calibrate the dynamic ranges for activations.\n",
    "def representative_data_gen():\n",
    "    for input_value in x_train: # Using x_train, which is already normalized and reshaped\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that input and output tensors are also quantized to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# Save the quantized TFLite model\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "with open(tflite_model_quant_filename, \"wb\") as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "print(f\"\\nQuantized TFLite model saved to {tflite_model_quant_filename}\")\n",
    "\n",
    "# Optional: Check the size of the quantized model\n",
    "tflite_model_quant_size_bytes = os.path.getsize(tflite_model_quant_filename)\n",
    "tflite_model_quant_size_kb = tflite_model_quant_size_bytes / 1024\n",
    "print(f\"Quantized TFLite Model Size: {tflite_model_quant_size_bytes} bytes ({tflite_model_quant_size_kb:.2f} KB)\")\n",
    "\n",
    "# Clean up the temporary .keras file\n",
    "os.remove(keras_model_path)\n",
    "print(f\"Cleaned up temporary Keras model file: {keras_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa5ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
