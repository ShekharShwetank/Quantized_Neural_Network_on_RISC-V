{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f001ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Shape of x_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f331a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train after reshaping: (60000, 28, 28, 1)\n",
      "Shape of x_test after reshaping: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Reshape images\n",
    "# Images are 28x28 grayscale, so we add a channel dimension of 1\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Shape of x_train after reshaping: {x_train.shape}\")\n",
    "print(f\"Shape of x_test after reshaping: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75feeec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min pixel value in x_train after normalization: 0.0\n",
      "Max pixel value in x_train after normalization: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Normalize pixel values\n",
    "# Convert pixel values from [0, 255] to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(f\"Min pixel value in x_train after normalization: {x_train.min()}\")\n",
    "print(f\"Max pixel value in x_train after normalization: {x_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6af7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train after one-hot encoding: (60000, 10)\n",
      "Shape of y_test after one-hot encoding: (10000, 10)\n",
      "Example of one-hot encoded label (first training label): [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: One-hot encode labels\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Shape of y_train after one-hot encoding: {y_train.shape}\")\n",
    "print(f\"Shape of y_test after one-hot encoding: {y_test.shape}\")\n",
    "print(f\"Example of one-hot encoded label (first training label): {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94261c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 6280      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6370 (24.88 KB)\n",
      "Trainable params: 6370 (24.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Network Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)), # Input layer: Flattens 28x28x1 images to a 784-element vector\n",
    "    Dense(8, activation='relu'),   # Hidden layer: 32 neurons with ReLU activation\n",
    "    Dense(10, activation='softmax')  # Output layer: 10 neurons (for 10 classes) with Softmax activation\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16e78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\n"
     ]
    }
   ],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b88313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 1.1253 - accuracy: 0.6677 - val_loss: 0.5332 - val_accuracy: 0.8645\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.8752 - val_loss: 0.3948 - val_accuracy: 0.8922\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8936 - val_loss: 0.3515 - val_accuracy: 0.9033\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3542 - accuracy: 0.9012 - val_loss: 0.3288 - val_accuracy: 0.9094\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.9072 - val_loss: 0.3148 - val_accuracy: 0.9127\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.9108 - val_loss: 0.3013 - val_accuracy: 0.9166\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.9142 - val_loss: 0.2951 - val_accuracy: 0.9184\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.9164 - val_loss: 0.2911 - val_accuracy: 0.9188\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.9183 - val_loss: 0.2839 - val_accuracy: 0.9199\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 0s 1ms/step - loss: 0.2862 - accuracy: 0.9206 - val_loss: 0.2817 - val_accuracy: 0.9223\n",
      "\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,        # Number of times to iterate over the entire training dataset\n",
    "    batch_size=256,   # Number of samples per gradient update\n",
    "    validation_data=(x_test, y_test) # Data to evaluate the loss and metrics on at the end of each epoch\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc97f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n",
      "Test Loss: 0.2817\n",
      "Test Accuracy: 0.9223\n",
      "\n",
      "Making predictions on a few test images...\n",
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference test completed. Predicted vs. True labels for 5 samples displayed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict a few test images\n",
    "print(\"\\nMaking predictions on a few test images...\")\n",
    "predictions = model.predict(x_test[:5]) # Predict on the first 5 test images\n",
    "\n",
    "# Display images and predictions\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {np.argmax(y_test[i])}\\nPred: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInference test completed. Predicted vs. True labels for 5 samples displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08091bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model File Size: 102336 bytes (99.94 KB, 0.10 MB)\n",
      "Total Model Parameters: 6370\n",
      "Estimated Memory for Weights (float32): 25480 bytes (24.88 KB, 0.02 MB)\n",
      "Estimated Peak Activation Memory for single inference: 6824 bytes (6.66 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint\n",
    "# Save the model to a temporary file to get its size on disk\n",
    "model_filename = \"mnist_baseline_model.h5\"\n",
    "model.save(model_filename)\n",
    "model_size_bytes = os.path.getsize(model_filename)\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "model_size_mb = model_size_kb / 1024\n",
    "\n",
    "print(f\"\\nModel File Size: {model_size_bytes} bytes ({model_size_kb:.2f} KB, {model_size_mb:.2f} MB)\")\n",
    "\n",
    "# To remove the temporary file after checking its size\n",
    "os.remove(model_filename)\n",
    "\n",
    "# Estimate memory for inference (weights and activations)\n",
    "# Parameters (weights and biases) are already calculated in model.summary()\n",
    "total_params = model.count_params()\n",
    "# Assuming float32 (4 bytes per parameter)\n",
    "weights_memory_bytes = total_params * 4\n",
    "weights_memory_kb = weights_memory_bytes / 1024\n",
    "weights_memory_mb = weights_memory_kb / 1024\n",
    "\n",
    "print(f\"Total Model Parameters: {total_params}\")\n",
    "print(f\"Estimated Memory for Weights (float32): {weights_memory_bytes} bytes ({weights_memory_kb:.2f} KB, {weights_memory_mb:.2f} MB)\")\n",
    "\n",
    "# Estimating activation memory for a single inference:\n",
    "# Input layer: 28*28*1 * 4 bytes/pixel (float32)\n",
    "# Flatten layer output: 784 * 4 bytes\n",
    "# Dense layer 1 output: 128 * 4 bytes\n",
    "# Dense layer 2 output: 10 * 4 bytes\n",
    "input_mem = 28 * 28 * 1 * 4\n",
    "flatten_output_mem = 784 * 4\n",
    "dense1_output_mem = 128 * 4\n",
    "dense2_output_mem = 10 * 4\n",
    "total_activation_mem_single_inference = input_mem + flatten_output_mem + dense1_output_mem + dense2_output_mem\n",
    "\n",
    "print(f\"Estimated Peak Activation Memory for single inference: {total_activation_mem_single_inference} bytes ({total_activation_mem_single_inference / 1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15769287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model File Size: 102336 bytes (99.94 KB, 0.10 MB)\n",
      "\n",
      "Keras model saved to mnist_baseline_model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpm76x_1c3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpm76x_1c3\\assets\n",
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized TFLite model saved to mnist_quantized_model.tflite\n",
      "Quantized TFLite Model Size: 8736 bytes (8.53 KB)\n",
      "Cleaned up temporary Keras model file: mnist_baseline_model.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint\n",
    "# Save the model to a temporary file to get its size on disk\n",
    "model_filename = \"mnist_baseline_model.h5\"\n",
    "model.save(model_filename)\n",
    "model_size_bytes = os.path.getsize(model_filename)\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "model_size_mb = model_size_kb / 1024\n",
    "\n",
    "print(f\"\\nModel File Size: {model_size_bytes} bytes ({model_size_kb:.2f} KB, {model_size_mb:.2f} MB)\")\n",
    "\n",
    "# To remove the temporary file after checking its size\n",
    "os.remove(model_filename)\n",
    "\n",
    "# Convert the Keras model to a TensorFlow Lite model with full integer quantization\n",
    "\n",
    "# Re-save the model in the native Keras format (recommended by TensorFlow warning)\n",
    "keras_model_path = \"mnist_baseline_model.keras\"\n",
    "model.save(keras_model_path)\n",
    "print(f\"\\nKeras model saved to {keras_model_path}\")\n",
    "\n",
    "# Load the model from the native Keras format for conversion\n",
    "loaded_model = keras.models.load_model(keras_model_path)\n",
    "\n",
    "# Create a TensorFlow Lite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "\n",
    "# Enable full integer quantization\n",
    "# This will quantize weights and activations to int8.\n",
    "# It requires a representative dataset for calibration to determine min/max ranges for activations.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Provide a representative dataset for calibration\n",
    "# The representative dataset should consist of a small subset of your training data.\n",
    "# TFLite uses this to calibrate the dynamic ranges for activations.\n",
    "def representative_data_gen():\n",
    "    for input_value in x_train: # Using x_train, which is already normalized and reshaped\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that input and output tensors are also quantized to int8\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert the model\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# Save the quantized TFLite model\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "with open(tflite_model_quant_filename, \"wb\") as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "print(f\"\\nQuantized TFLite model saved to {tflite_model_quant_filename}\")\n",
    "\n",
    "# Optional: Check the size of the quantized model\n",
    "tflite_model_quant_size_bytes = os.path.getsize(tflite_model_quant_filename)\n",
    "tflite_model_quant_size_kb = tflite_model_quant_size_bytes / 1024\n",
    "print(f\"Quantized TFLite Model Size: {tflite_model_quant_size_bytes} bytes ({tflite_model_quant_size_kb:.2f} KB)\")\n",
    "\n",
    "# Clean up the temporary .keras file\n",
    "os.remove(keras_model_path)\n",
    "print(f\"Cleaned up temporary Keras model file: {keras_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1aa5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C header file generated: mnist_model_data.h\n",
      "This file contains the quantized model as a byte array, ready for embedded C inclusion.\n"
     ]
    }
   ],
   "source": [
    "# Generate a .h file for the quantized TFLite model\n",
    "\n",
    "import os\n",
    "\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "c_model_filename = \"mnist_model_data.h\"\n",
    "\n",
    "# Read the TFLite model file as bytes\n",
    "with open(tflite_model_quant_filename, \"rb\") as f:\n",
    "    tflite_model_bytes = f.read()\n",
    "\n",
    "# Convert bytes to a C-style array declaration\n",
    "# Format: const unsigned char model_name[] = { 0x.., 0x.., ... };\n",
    "bytes_as_hex = [f\"0x{byte:02x}\" for byte in tflite_model_bytes]\n",
    "c_array_declaration = f\"const unsigned char {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_data[] = {{\\n    \"\n",
    "c_array_declaration += \", \".join(bytes_as_hex)\n",
    "c_array_declaration += \"\\n};\\n\"\n",
    "c_array_declaration += f\"const int {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_len = {len(tflite_model_bytes)};\\n\"\n",
    "\n",
    "# Write the C array declaration to the .h file\n",
    "with open(c_model_filename, \"w\") as f:\n",
    "    f.write(\"#ifndef MNIST_MODEL_DATA_H_\\n\")\n",
    "    f.write(\"#define MNIST_MODEL_DATA_H_\\n\\n\")\n",
    "    f.write(c_array_declaration)\n",
    "    f.write(\"\\n#endif // MNIST_MODEL_DATA_H_\\n\")\n",
    "\n",
    "print(f\"\\nC header file generated: {c_model_filename}\")\n",
    "print(f\"This file contains the quantized model as a byte array, ready for embedded C inclusion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a04561c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TFLite model from: mnist_quantized_model.tflite\n",
      "Inspecting operator: op_name=RESHAPE, builtin_code=None, inputs=[0 1], outputs=[6]\n",
      "Inspecting operator: op_name=FULLY_CONNECTED, builtin_code=None, inputs=[6 5 4], outputs=[7]\n",
      "Processing FULLY_CONNECTED layer 1...\n",
      "Inspecting operator: op_name=FULLY_CONNECTED, builtin_code=None, inputs=[7 3 2], outputs=[8]\n",
      "Processing FULLY_CONNECTED layer 2...\n",
      "Inspecting operator: op_name=SOFTMAX, builtin_code=None, inputs=[8], outputs=[9]\n",
      "Inspecting operator: op_name=DELEGATE, builtin_code=None, inputs=[0 1 2 3 4 5], outputs=[8]\n",
      "Generating quantized sample inputs and labels...\n",
      "Generated mnist_model_params.h and mnist_model_params.c with model parameters and sample inputs.\n",
      "Generated mnist_model_params.h and mnist_model_params.c with model parameters and sample inputs.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your quantized TFLite model generated by mnist_baseline_model.ipynb\n",
    "TFLITE_MODEL_PATH = \"mnist_quantized_model.tflite\"\n",
    "\n",
    "# Output C header and source files\n",
    "C_HEADER_FILE = \"mnist_model_params.h\"\n",
    "C_SOURCE_FILE = \"mnist_model_params.c\"\n",
    "\n",
    "# --- Helper function for quantization of input data ---\n",
    "def quantize_input(image_data_float32, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Quantizes float32 input data to int8 using the given scale and zero_point.\n",
    "    Formula: Q = round(R / S + Z)\n",
    "    \"\"\"\n",
    "    quantized_data = np.round(image_data_float32 / scale + zero_point)\n",
    "    # Clip to int8 range [-128, 127]\n",
    "    quantized_data = np.clip(quantized_data, -128, 127).astype(np.int8)\n",
    "    return quantized_data\n",
    "\n",
    "# --- Main generation function ---\n",
    "def generate_c_arrays_from_tflite(model_path, header_file, source_file):\n",
    "    print(f\"Loading TFLite model from: {model_path}\")\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "    # Get operator details. This method is usually reliable.\n",
    "    # The structure of elements in this list can vary by TF Lite version.\n",
    "    # Each element is a dictionary representing an operator.\n",
    "    ops_details_raw = interpreter._get_ops_details()\n",
    "\n",
    "    # Open header and source files\n",
    "    with open(header_file, \"w\") as hf, open(source_file, \"w\") as sf:\n",
    "        hf.write(\"/*\\n\")\n",
    "        hf.write(\" * This file is auto-generated by generate_c_model_params.py.\\n\")\n",
    "        hf.write(\" * Do not modify manually.\\n\")\n",
    "        hf.write(\" */\\n\")\n",
    "        hf.write(\"#ifndef MNIST_MODEL_PARAMS_H\\n\")\n",
    "        hf.write(\"#define MNIST_MODEL_PARAMS_H\\n\\n\")\n",
    "        hf.write(\"#include <stdint.h>\\n\\n\")\n",
    "\n",
    "        sf.write(\"/*\\n\")\n",
    "        sf.write(\" * This file is auto-generated by generate_c_model_params.py.\\n\")\n",
    "        sf.write(\" * Do not modify manually.\\n\")\n",
    "        sf.write(\" */\\n\")\n",
    "        sf.write(\"#include \\\"mnist_model_params.h\\\"\\n\\n\")\n",
    "\n",
    "        # Define MAX_N_ACTIVATIONS based on the model's input size (28*28 for MNIST)\n",
    "        hf.write(f\"#define MAX_N_ACTIVATIONS {input_details[0]['shape'][1] * input_details[0]['shape'][2]}\\n\\n\")\n",
    "\n",
    "        layer_idx = 1\n",
    "        for op_detail in ops_details_raw:\n",
    "            # Safely get operator name and builtin code\n",
    "            current_op_name = op_detail.get('op_name')\n",
    "            current_builtin_code = op_detail.get('builtin_code')\n",
    "\n",
    "            print(f\"Inspecting operator: op_name={current_op_name}, builtin_code={current_builtin_code}, inputs={op_detail.get('inputs')}, outputs={op_detail.get('outputs')}\")\n",
    "\n",
    "            # Check if it's a FULLY_CONNECTED layer\n",
    "            # BuiltinOperator.FULLY_CONNECTED typically has builtin_code 9\n",
    "            # or op_name 'FULLY_CONNECTED'\n",
    "            is_fully_connected = False\n",
    "            if current_op_name == 'FULLY_CONNECTED':\n",
    "                is_fully_connected = True\n",
    "            elif current_builtin_code == 9: # Known builtin_code for FULLY_CONNECTED\n",
    "                is_fully_connected = True\n",
    "\n",
    "            if is_fully_connected:\n",
    "                print(f\"Processing FULLY_CONNECTED layer {layer_idx}...\")\n",
    "\n",
    "                # Input, Weights, Bias tensors are identified by their indices in op_detail['inputs']\n",
    "                # TFLite FULLY_CONNECTED op inputs: [input_tensor, weights_tensor, bias_tensor (optional)]\n",
    "                input_tensor_idx = op_detail['inputs'][0]\n",
    "                weights_tensor_idx = op_detail['inputs'][1]\n",
    "                biases_tensor_idx = op_detail['inputs'][2] if len(op_detail['inputs']) > 2 else -1\n",
    "                output_tensor_idx = op_detail['outputs'][0]\n",
    "\n",
    "                # Find the corresponding tensor details\n",
    "                input_tensor = next(t for t in tensor_details if t['index'] == input_tensor_idx)\n",
    "                weights_tensor = next(t for t in tensor_details if t['index'] == weights_tensor_idx)\n",
    "                output_tensor = next(t for t in tensor_details if t['index'] == output_tensor_idx)\n",
    "\n",
    "                # Extract actual data\n",
    "                weights_data = interpreter.tensor(weights_tensor_idx)()\n",
    "                biases_data = interpreter.tensor(biases_tensor_idx)() if biases_tensor_idx != -1 else None\n",
    "\n",
    "                # Extract quantization parameters\n",
    "                # Use .get() and check for empty lists to prevent IndexError\n",
    "                input_scale = input_tensor['quantization_parameters'].get('scales', [1.0])[0]\n",
    "                input_zero_point = input_tensor['quantization_parameters'].get('zero_points', [0])[0]\n",
    "                output_scale = output_tensor['quantization_parameters'].get('scales', [1.0])[0]\n",
    "                output_zero_point = output_tensor['quantization_parameters'].get('zero_points', [0])[0]\n",
    "                weights_scale = weights_tensor['quantization_parameters'].get('scales', [1.0])[0]\n",
    "                weights_zero_point = weights_tensor['quantization_parameters'].get('zero_points', [0])[0]\n",
    "\n",
    "                # Determine incoming and outgoing weights (dimensions)\n",
    "                incoming_weights = weights_tensor['shape'][1]\n",
    "                outgoing_weights = weights_tensor['shape'][0]\n",
    "\n",
    "                # Pack 8-bit weights into uint32_t. Each uint32_t holds 4 int8 weights.\n",
    "                packed_weights = []\n",
    "                flat_weights = weights_data.flatten()\n",
    "                # Pad with zeros if not a multiple of 4\n",
    "                if len(flat_weights) % 4 != 0:\n",
    "                    flat_weights = np.pad(flat_weights, (0, 4 - (len(flat_weights) % 4)), 'constant', constant_values=0)\n",
    "\n",
    "                for i in range(0, len(flat_weights), 4):\n",
    "                    val = 0\n",
    "                    for j in range(4):\n",
    "                        # Convert int8 to unsigned byte for packing, then shift\n",
    "                        # Ensure the value is treated as unsigned before shifting for packing\n",
    "                        val |= (np.uint32(flat_weights[i+j].astype(np.uint8)) << (24 - j*8))\n",
    "                    packed_weights.append(f\"0x{val:08X}\")\n",
    "\n",
    "                # Biases are typically int32 for quantized models\n",
    "                biases_hex = [f\"{b}\" for b in biases_data] if biases_data is not None else [] # Use decimal for int32_t\n",
    "\n",
    "                # Write declarations to header\n",
    "                hf.write(f\"// Layer {layer_idx} Parameters\\n\")\n",
    "                hf.write(f\"extern const uint32_t L{layer_idx}_weights[{len(packed_weights)}];\\n\")\n",
    "                if biases_data is not None:\n",
    "                    hf.write(f\"extern const int32_t L{layer_idx}_biases[{len(biases_hex)}];\\n\")\n",
    "                hf.write(f\"extern const int32_t L{layer_idx}_bitperweight;\\n\")\n",
    "                hf.write(f\"extern const uint32_t L{layer_idx}_incoming_weights;\\n\")\n",
    "                hf.write(f\"extern const uint32_t L{layer_idx}_outgoing_weights;\\n\")\n",
    "                hf.write(f\"extern const float L{layer_idx}_input_scale;\\n\")\n",
    "                hf.write(f\"extern const int32_t L{layer_idx}_input_zero_point;\\n\")\n",
    "                hf.write(f\"extern const float L{layer_idx}_output_scale;\\n\")\n",
    "                hf.write(f\"extern const int32_t L{layer_idx}_output_zero_point;\\n\")\n",
    "                hf.write(f\"extern const float L{layer_idx}_weights_scale;\\n\")\n",
    "                hf.write(f\"extern const int32_t L{layer_idx}_weights_zero_point;\\n\\n\")\n",
    "\n",
    "                # Write definitions to source file\n",
    "                sf.write(f\"// Layer {layer_idx} Parameters\\n\")\n",
    "                sf.write(f\"const uint32_t L{layer_idx}_weights[{len(packed_weights)}] = {{\\n    {', '.join(packed_weights)}\\n}};\\n\")\n",
    "                if biases_data is not None:\n",
    "                    sf.write(f\"const int32_t L{layer_idx}_biases[{len(biases_hex)}] = {{\\n    {', '.join(biases_hex)}\\n}};\\n\")\n",
    "                sf.write(f\"const int32_t L{layer_idx}_bitperweight = 8 + 8;\\n\") # Hardcoding to 8+8 for TFLite int8\n",
    "                sf.write(f\"const uint32_t L{layer_idx}_incoming_weights = {incoming_weights};\\n\")\n",
    "                sf.write(f\"const uint32_t L{layer_idx}_outgoing_weights = {outgoing_weights};\\n\")\n",
    "                sf.write(f\"const float L{layer_idx}_input_scale = {input_scale:.8f}f;\\n\")\n",
    "                sf.write(f\"const int32_t L{layer_idx}_input_zero_point = {input_zero_point};\\n\")\n",
    "                sf.write(f\"const float L{layer_idx}_output_scale = {output_scale:.8f}f;\\n\")\n",
    "                sf.write(f\"const int32_t L{layer_idx}_output_zero_point = {output_zero_point};\\n\")\n",
    "                sf.write(f\"const float L{layer_idx}_weights_scale = {weights_scale:.8f}f;\\n\")\n",
    "                sf.write(f\"const int32_t L{layer_idx}_weights_zero_point = {weights_zero_point};\\n\\n\")\n",
    "\n",
    "                layer_idx += 1\n",
    "\n",
    "        # Generate quantized sample inputs and labels\n",
    "        print(\"Generating quantized sample inputs and labels...\")\n",
    "        # Load original MNIST test data (from Keras/TensorFlow) for true input images\n",
    "        (_, _), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "        # Preprocess as done in the notebook: normalize to [0, 1] then flatten\n",
    "        x_test_processed = x_test_raw.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "        # Get input quantization parameters from the *quantized model's* input tensor\n",
    "        # This is the input tensor of the *first* layer in the TFLite model\n",
    "        model_input_details = interpreter.get_input_details()[0]\n",
    "        model_input_scale = model_input_details['quantization_parameters'].get('scales', [1.0])[0]\n",
    "        model_input_zero_point = model_input_details['quantization_parameters'].get('zero_points', [0])[0]\n",
    "\n",
    "        num_samples_to_generate = 4 # As used in main.c\n",
    "        hf.write(\"// Quantized sample input images and their labels\\n\")\n",
    "        sf.write(\"// Quantized sample input images and their labels\\n\")\n",
    "\n",
    "        for i in range(num_samples_to_generate):\n",
    "            # Flatten the preprocessed float32 image\n",
    "            original_image_float = x_test_processed[i].flatten() # This will be 784 elements\n",
    "\n",
    "            # Quantize\n",
    "            quantized_image_int8 = quantize_input(original_image_float, model_input_scale, model_input_zero_point)\n",
    "\n",
    "            label = y_test_raw[i]\n",
    "\n",
    "            # Format for C array\n",
    "            image_c_array_content = ', '.join([f\"{val}\" for val in quantized_image_int8])\n",
    "\n",
    "            hf.write(f\"extern const int8_t input_data_{i}[{len(quantized_image_int8)}];\\n\")\n",
    "            hf.write(f\"extern const uint8_t label_{i};\\n\")\n",
    "            sf.write(f\"const int8_t input_data_{i}[{len(quantized_image_int8)}] = {{\\n    {image_c_array_content}\\n}};\\n\")\n",
    "            sf.write(f\"const uint8_t label_{i} = {label};\\n\")\n",
    "\n",
    "        hf.write(\"\\n#endif // MNIST_MODEL_PARAMS_H\\n\")\n",
    "\n",
    "    print(f\"Generated {header_file} and {source_file} with model parameters and sample inputs.\")\n",
    "\n",
    "# --- Script execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(TFLITE_MODEL_PATH):\n",
    "        print(f\"Error: Quantized TFLite model not found at {TFLITE_MODEL_PATH}.\")\n",
    "        print(\"Please ensure you have run the `mnist_baseline_model.ipynb` notebook\")\n",
    "        print(\"to generate `mnist_quantized_model.tflite` before running this script.\")\n",
    "    else:\n",
    "        generate_c_arrays_from_tflite(TFLITE_MODEL_PATH, C_HEADER_FILE, C_SOURCE_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
