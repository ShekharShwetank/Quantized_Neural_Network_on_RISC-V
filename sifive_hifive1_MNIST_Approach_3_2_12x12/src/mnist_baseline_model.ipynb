{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e50e711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dcd3a467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x2500 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3ea02654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport numpy as np\\nfrom skimage.transform import resize\\n\\ndef preprocess_image(image_28x28):\\n    # Normalize pixel values\\n    image = image_28x28.astype('float32') / 255.0\\n    \\n    # Find bounding box\\n    rows = np.any(image, axis=1)\\n    cols = np.any(image, axis=0)\\n    ymin, ymax = np.where(rows)[0][[0, -1]]\\n    xmin, xmax = np.where(cols)[0][[0, -1]]\\n\\n    # Add a small buffer to the crop\\n    buffer = 0\\n    ymin = max(0, ymin - buffer)\\n    ymax = min(28, ymax + buffer)\\n    xmin = max(0, xmin - buffer)\\n    xmax = min(28, xmax + buffer)\\n\\n    # Crop the image\\n    cropped_image = image[ymin:ymax, xmin:xmax]\\n\\n    # Resize the cropped image to 8x8\\n    resized_8x8 = resize(cropped_image, (12, 12), anti_aliasing=True)\\n\\n    # Binarize the resized image\\n    final_8x8_binary = (resized_8x8 > 0.4).astype('float32')\\n    \\n    return final_8x8_binary\\n\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "def preprocess_image(image_28x28):\n",
    "    # Normalize pixel values\n",
    "    image = image_28x28.astype('float32') / 255.0\n",
    "    \n",
    "    # Find bounding box\n",
    "    rows = np.any(image, axis=1)\n",
    "    cols = np.any(image, axis=0)\n",
    "    ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "    xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Add a small buffer to the crop\n",
    "    buffer = 0\n",
    "    ymin = max(0, ymin - buffer)\n",
    "    ymax = min(28, ymax + buffer)\n",
    "    xmin = max(0, xmin - buffer)\n",
    "    xmax = min(28, xmax + buffer)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Resize the cropped image to 8x8\n",
    "    resized_8x8 = resize(cropped_image, (12, 12), anti_aliasing=True)\n",
    "\n",
    "    # Binarize the resized image\n",
    "    final_8x8_binary = (resized_8x8 > 0.4).astype('float32')\n",
    "    \n",
    "    return final_8x8_binary\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ed708897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "\n",
    "def preprocess_image(image_28x28):\n",
    "    # Normalize pixel values to a range of 0 to 1\n",
    "    image_float = image_28x28.astype('float32') / 255.0\n",
    "\n",
    "    # Find the bounding box\n",
    "    rows = np.any(image_float > 0.1, axis=1) # Use a small threshold\n",
    "    cols = np.any(image_float > 0.1, axis=0)\n",
    "    ymin, ymax = np.where(rows)[0][[0, -1]]\n",
    "    xmin, xmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Add a small buffer and ensure bounds are valid\n",
    "    buffer = 2\n",
    "    ymin = max(0, ymin - buffer)\n",
    "    ymax = min(28, ymax + buffer)\n",
    "    xmin = max(0, xmin - buffer)\n",
    "    xmax = min(28, xmax + buffer)\n",
    "    \n",
    "    # Crop the image\n",
    "    cropped_image = image_float[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    # Resize the cropped image to the modelâ€™s input size (12x12)\n",
    "    resized_image = tf.image.resize(\n",
    "        np.expand_dims(cropped_image, axis=-1), \n",
    "        (12, 12), \n",
    "        method='bilinear'\n",
    "    ).numpy().squeeze()\n",
    "\n",
    "    # Apply a fixed binary threshold for a binary representation\n",
    "    final_image_binary = (resized_image > 0.4).astype('float32')\n",
    "\n",
    "    return final_image_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0176fbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training data shape: (60000, 12, 12)\n",
      "Final testing data shape: (10000, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Process the entire training and testing datasets\n",
    "x_train_processed = np.array([preprocess_image(img) for img in x_train])\n",
    "x_test_processed = np.array([preprocess_image(img) for img in x_test])\n",
    "\n",
    "# Reshape the data for the model's input layer\n",
    "# Add a channel dimension for the Flatten layer (batch, 8, 8, 1)\n",
    "#x_train_processed = x_train_processed.reshape(-1, 12, 12, 1)\n",
    "#x_test_processed = x_test_processed.reshape(-1, 12, 12, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_one_hot = to_categorical(y_train, 10)\n",
    "y_test_one_hot = to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Final training data shape: {x_train_processed.shape}\")\n",
    "print(f\"Final testing data shape: {x_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f94261c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define the Neural Network Model\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Flatten\\nfrom keras.regularizers import l1\\n\\n# Define optimized model for SiFive HiFive1\\nmodel = Sequential([\\n    Flatten(input_shape=(12, 12, 1)),\\n    # Increase neurons while staying within memory constraints\\n    Dense(64, activation='relu',\\n          kernel_regularizer=l1(0.005)),  # Reduced L1 penalty\\n          #kernel_constraint=tf.keras.constraints.MaxNorm(3)), # Add max norm constraint\\n    Dense(64, activation='relu',\\n          kernel_regularizer=l1(0.005)),\\n    Dense(10, activation='softmax')\\n])\\n\\nmodel.summary()\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Define the Neural Network Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Define optimized model for SiFive HiFive1\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(12, 12, 1)),\n",
    "    # Increase neurons while staying within memory constraints\n",
    "    Dense(64, activation='relu',\n",
    "          kernel_regularizer=l1(0.005)),  # Reduced L1 penalty\n",
    "          #kernel_constraint=tf.keras.constraints.MaxNorm(3)), # Add max norm constraint\n",
    "    Dense(64, activation='relu',\n",
    "          kernel_regularizer=l1(0.005)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e88e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_12 (Flatten)        (None, 144)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 64)                9280      \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14090 (55.04 KB)\n",
      "Trainable params: 14090 (55.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LeakyReLU\n",
    "from keras.regularizers import l1\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a better model with Leaky ReLU and no L1 regularization\n",
    "# This can help solve the 'dying ReLU' problem and zero weights\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(12, 12, 1)),\n",
    "    Dense(64),\n",
    "    LeakyReLU(alpha=0.1), # Use Leaky ReLU to prevent zero activations\n",
    "    Dense(64),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d16e78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\n"
     ]
    }
   ],
   "source": [
    "# Improved training configuration\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2b88313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.8088 - val_loss: 0.2819 - val_accuracy: 0.9159 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9186 - val_loss: 0.2358 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9320 - val_loss: 0.2121 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1996 - accuracy: 0.9410 - val_loss: 0.1839 - val_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1759 - accuracy: 0.9485 - val_loss: 0.1715 - val_accuracy: 0.9490 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1596 - accuracy: 0.9532 - val_loss: 0.1579 - val_accuracy: 0.9527 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1461 - accuracy: 0.9565 - val_loss: 0.1517 - val_accuracy: 0.9540 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9593 - val_loss: 0.1432 - val_accuracy: 0.9568 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9625 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9635 - val_loss: 0.1298 - val_accuracy: 0.9589 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9665 - val_loss: 0.1256 - val_accuracy: 0.9617 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9677 - val_loss: 0.1215 - val_accuracy: 0.9626 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.1000 - accuracy: 0.9701 - val_loss: 0.1184 - val_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.9716 - val_loss: 0.1168 - val_accuracy: 0.9629 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9731 - val_loss: 0.1159 - val_accuracy: 0.9633 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9745 - val_loss: 0.1159 - val_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.1133 - val_accuracy: 0.9651 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0785 - accuracy: 0.9762 - val_loss: 0.1059 - val_accuracy: 0.9655 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9777 - val_loss: 0.1105 - val_accuracy: 0.9649 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9791 - val_loss: 0.1071 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9790 - val_loss: 0.1047 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.1045 - val_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9810 - val_loss: 0.1045 - val_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9822 - val_loss: 0.1033 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9829 - val_loss: 0.1035 - val_accuracy: 0.9689 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9832 - val_loss: 0.1037 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0528 - accuracy: 0.9841 - val_loss: 0.1027 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9845 - val_loss: 0.1008 - val_accuracy: 0.9696 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9852 - val_loss: 0.1038 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9858 - val_loss: 0.1019 - val_accuracy: 0.9699 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9869 - val_loss: 0.1050 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.9873 - val_loss: 0.1093 - val_accuracy: 0.9686 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9876 - val_loss: 0.1049 - val_accuracy: 0.9701 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9880 - val_loss: 0.1091 - val_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9890 - val_loss: 0.1069 - val_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9887 - val_loss: 0.1095 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9892 - val_loss: 0.1074 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9898 - val_loss: 0.1100 - val_accuracy: 0.9690 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 0.1143 - val_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 40/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.9904 - val_loss: 0.1107 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 41/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.9912 - val_loss: 0.1125 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 42/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 0.1113 - val_accuracy: 0.9680 - lr: 0.0010\n",
      "Epoch 43/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 0.1157 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 44/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9918 - val_loss: 0.1234 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 45/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.9919 - val_loss: 0.1200 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 46/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9921 - val_loss: 0.1190 - val_accuracy: 0.9671 - lr: 0.0010\n",
      "Epoch 47/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9934 - val_loss: 0.1253 - val_accuracy: 0.9669 - lr: 0.0010\n",
      "Epoch 48/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9931 - val_loss: 0.1225 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 49/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9935 - val_loss: 0.1232 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 50/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1235 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 51/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9942 - val_loss: 0.1355 - val_accuracy: 0.9655 - lr: 0.0010\n",
      "Epoch 52/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9937 - val_loss: 0.1253 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 53/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9946 - val_loss: 0.1371 - val_accuracy: 0.9665 - lr: 0.0010\n",
      "Epoch 54/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.1301 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 55/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9950 - val_loss: 0.1332 - val_accuracy: 0.9653 - lr: 0.0010\n",
      "Epoch 56/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.1355 - val_accuracy: 0.9663 - lr: 0.0010\n",
      "Epoch 57/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9952 - val_loss: 0.1480 - val_accuracy: 0.9662 - lr: 0.0010\n",
      "Epoch 58/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9950 - val_loss: 0.1345 - val_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 59/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9961 - val_loss: 0.1405 - val_accuracy: 0.9680 - lr: 0.0010\n",
      "Epoch 60/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9960 - val_loss: 0.1429 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 61/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9964 - val_loss: 0.1387 - val_accuracy: 0.9690 - lr: 0.0010\n",
      "Epoch 62/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9963 - val_loss: 0.1488 - val_accuracy: 0.9667 - lr: 0.0010\n",
      "Epoch 63/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 0.1407 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 64/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.1509 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 65/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.1468 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 66/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9973 - val_loss: 0.1586 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 67/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.1503 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 68/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.1541 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 69/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.1516 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 70/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 0.1536 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 71/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 0.1564 - val_accuracy: 0.9671 - lr: 0.0010\n",
      "Epoch 72/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.1547 - val_accuracy: 0.9678 - lr: 0.0010\n",
      "Epoch 73/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.1622 - val_accuracy: 0.9667 - lr: 0.0010\n",
      "Epoch 74/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.1603 - val_accuracy: 0.9686 - lr: 0.0010\n",
      "Epoch 75/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.0076 - accuracy: 0.9988 - val_loss: 0.1673 - val_accuracy: 0.9677 - lr: 0.0010\n",
      "Epoch 76/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 0.1615 - val_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 77/100\n",
      "236/236 [==============================] - 0s 1ms/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.1675 - val_accuracy: 0.9663 - lr: 0.0010\n",
      "Epoch 78/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.1729 - val_accuracy: 0.9665 - lr: 0.0010\n",
      "Epoch 79/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9984 - val_loss: 0.1741 - val_accuracy: 0.9663 - lr: 0.0010\n",
      "Epoch 80/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.1780 - val_accuracy: 0.9664 - lr: 0.0010\n",
      "Epoch 81/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.1738 - val_accuracy: 0.9674 - lr: 0.0010\n",
      "Epoch 82/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9966 - val_loss: 0.1704 - val_accuracy: 0.9676 - lr: 0.0010\n",
      "Epoch 83/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9984 - val_loss: 0.1735 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 84/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.1765 - val_accuracy: 0.9657 - lr: 0.0010\n",
      "Epoch 85/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.1776 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 86/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.1800 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 87/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.1835 - val_accuracy: 0.9670 - lr: 0.0010\n",
      "Epoch 88/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.1786 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.1812 - val_accuracy: 0.9672 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.1966 - val_accuracy: 0.9669 - lr: 0.0010\n",
      "Epoch 91/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.2153 - val_accuracy: 0.9645 - lr: 0.0010\n",
      "Epoch 92/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.1953 - val_accuracy: 0.9668 - lr: 0.0010\n",
      "Epoch 93/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.1835 - val_accuracy: 0.9686 - lr: 0.0010\n",
      "Epoch 94/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.1976 - val_accuracy: 0.9664 - lr: 0.0010\n",
      "Epoch 95/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.1964 - val_accuracy: 0.9665 - lr: 0.0010\n",
      "Epoch 96/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.1935 - val_accuracy: 0.9685 - lr: 0.0010\n",
      "Epoch 97/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9999 - val_loss: 0.1913 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 98/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1942 - val_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 99/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.2004 - val_accuracy: 0.9673 - lr: 0.0010\n",
      "Epoch 100/100\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.2353 - val_accuracy: 0.9625 - lr: 0.0010\n",
      "\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train with more epochs and data augmentation\n",
    "history = model.fit(\n",
    "    x_train_processed, y_train_one_hot,  # Use processed data and one-hot encoded labels\n",
    "    epochs=100,\n",
    "    batch_size=255,\n",
    "    validation_data=(x_test_processed, y_test_one_hot),  # Use processed test data\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ffc97f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n",
      "Test Loss: 0.2353\n",
      "Test Accuracy: 0.9625\n",
      "\n",
      "Making predictions on a few test images...\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x2500 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference test completed. Predicted vs. True labels for 25 samples displayed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "loss, accuracy = model.evaluate(x_test_processed, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict a few test images\n",
    "print(\"\\nMaking predictions on a few test images...\")\n",
    "predictions = model.predict(x_test_processed[:25])  # Use processed test data\n",
    "\n",
    "# Display images and predictions\n",
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    plt.imshow(x_test_processed[i].reshape(12, 12))\n",
    "    plt.title(f\"True: {np.argmax(y_test_one_hot[i])}\\nPred: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInference test completed. Predicted vs. True labels for 25 samples displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "08091bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Analysis for SiFive FE310-G002 Board:\n",
      "Available Memory:\n",
      "- Instruction Cache (L1): 16KB\n",
      "- Data SRAM (L1): 16KB\n",
      "- ROM: 8KB\n",
      "- On-board SPI Flash: 32 Mbit\n",
      "\n",
      "Model Memory Requirements:\n",
      "Total Model Parameters: 14090\n",
      "Estimated Memory for Weights (INT8): 14090 bytes (13.76 KB)\n",
      "Estimated Activation Memory: 426 bytes (0.42 KB)\n",
      "Total Memory Requirement: 14516 bytes (14.18 KB)\n",
      "\n",
      "Data SRAM Utilization: 88.6% of 16KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint for SiFive FE310-G002\n",
    "print(\"\\nMemory Analysis for SiFive FE310-G002 Board:\")\n",
    "print(\"Available Memory:\")\n",
    "print(\"- Instruction Cache (L1): 16KB\")\n",
    "print(\"- Data SRAM (L1): 16KB\")\n",
    "print(\"- ROM: 8KB\")\n",
    "print(\"- On-board SPI Flash: 32 Mbit\")\n",
    "\n",
    "# Calculate model parameters memory (using INT8 = 1 byte per parameter)\n",
    "total_params = model.count_params()\n",
    "weights_memory_bytes = total_params  # 8 bits (1 byte) per parameter for INT8\n",
    "weights_memory_kb = weights_memory_bytes / 1024\n",
    "\n",
    "print(f\"\\nModel Memory Requirements:\")\n",
    "print(f\"Total Model Parameters: {total_params}\")\n",
    "print(f\"Estimated Memory for Weights (INT8): {weights_memory_bytes:.0f} bytes ({weights_memory_kb:.2f} KB)\")\n",
    "\n",
    "# Calculate activation memory for inference (all INT8)\n",
    "input_layer_mem = 12 * 12 * 1    # Input layer (12x12x1 INT8)\n",
    "flatten_mem = 144               # Flattened input (144 INT8)\n",
    "dense1_mem = 64                # First dense layer output (64 INT8)\n",
    "dense2_mem = 64                # Second dense layer output (64 INT8)\n",
    "output_mem = 10                # Output layer (10 INT8)\n",
    "\n",
    "# Sum up all activation memory\n",
    "total_activation_mem = (\n",
    "    input_layer_mem +  # Input activations\n",
    "    flatten_mem +      # Flattened layer\n",
    "    dense1_mem +       # First dense layer\n",
    "    dense2_mem +       # Second dense layer\n",
    "    output_mem         # Output layer\n",
    ")\n",
    "\n",
    "print(f\"Estimated Activation Memory: {total_activation_mem} bytes ({total_activation_mem/1024:.2f} KB)\")\n",
    "\n",
    "# Total memory requirement\n",
    "total_memory = weights_memory_bytes + total_activation_mem\n",
    "print(f\"Total Memory Requirement: {total_memory:.0f} bytes ({total_memory/1024:.2f} KB)\")\n",
    "\n",
    "# Memory utilization analysis\n",
    "data_sram_utilization = (total_memory / (16 * 1024)) * 100\n",
    "print(f\"\\nData SRAM Utilization: {data_sram_utilization:.1f}% of 16KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "15769287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport os\\nimport tensorflow as tf\\n\\n# Save the model in Keras format\\nkeras_model_path = \"mnist_baseline_model.keras\"\\nmodel.save(keras_model_path)\\nprint(f\"\\nKeras model saved to {keras_model_path}\")\\n\\n# Load the model for conversion\\nloaded_model = tf.keras.models.load_model(keras_model_path)\\n\\n# Create TFLite converteconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\\n# Create TFLite converter with INT4 quantization\\n# Enhanced quantization configuration\\nconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\n\\n# Set target spec for INT4\\nconverter.target_spec.supported_ops = [\\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\\n    tf.lite.OpsSet.TFLITE_BUILTINS,\\n]\\n\\n# Increase calibration samples\\ndef representative_data_gen():\\n    num_calibration_samples = 1000  # Increased from 100\\n    for i in range(num_calibration_samples):\\n        # Add noise for robustness\\n        image = x_train_processed[i:i+1].astype(np.float32)\\n        image += np.random.normal(0, 0.01, image.shape)\\n        image = np.clip(image, 0, 1)\\n        yield [image]\\n\\nconverter.representative_dataset = representative_data_gen\\nconverter.inference_input_type = tf.int8\\nconverter.inference_output_type = tf.int8\\n\\n\\n# Force 4-bit weights through post-training quantization\\nconverter.target_spec._experimental_low_bit_qat = True\\nconverter.target_spec._experimental_weight_bits = 4\\n\\n# Convert model to TFLite format\\ntflite_model_quant = converter.convert()\\n\\n# Save the quantized model\\ntflite_model_quant_filename = \"mnist_quantized_model.tflite\"\\nwith open(tflite_model_quant_filename, \"wb\") as f:\\n    f.write(tflite_model_quant)\\n\\nprint(f\"\\nInt4 quantized TFLite model saved to {tflite_model_quant_filename}\")\\n\\n# Check model size\\ntflite_model_quant_size_bytes = os.path.getsize(tflite_model_quant_filename)\\ntflite_model_quant_size_kb = tflite_model_quant_size_bytes / 1024\\nprint(f\"Int4 Quantized Model Size: {tflite_model_quant_size_bytes} bytes ({tflite_model_quant_size_kb:.2f} KB)\")\\n\\n# Clean up temporary files\\nos.remove(keras_model_path)\\nprint(f\"Cleaned up temporary Keras model file: {keras_model_path}\")\\n\\n# Create an interpreter to analyze the model\\ninterpreter = tf.lite.Interpreter(model_path=tflite_model_quant_filename)\\ninterpreter.allocate_tensors()\\n\\n# Get model details\\ninput_details = interpreter.get_input_details()\\noutput_details = interpreter.get_output_details()\\n\\nprint(\"\\nModel Details:\")\\nprint(\"Input:\", input_details)\\nprint(\"Output:\", output_details)\\n\\n# Print quantization parameters\\nprint(\"\\nQuantization Parameters:\")\\nprint(f\"Input Scale: {input_details[0][\\'quantization_parameters\\'][\\'scales\\']}\")\\nprint(f\"Input Zero Point: {input_details[0][\\'quantization_parameters\\'][\\'zero_points\\']}\")\\nprint(f\"Output Scale: {output_details[0][\\'quantization_parameters\\'][\\'scales\\']}\")\\nprint(f\"Output Zero Point: {output_details[0][\\'quantization_parameters\\'][\\'zero_points\\']}\")\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the model in Keras format\n",
    "keras_model_path = \"mnist_baseline_model.keras\"\n",
    "model.save(keras_model_path)\n",
    "print(f\"\\nKeras model saved to {keras_model_path}\")\n",
    "\n",
    "# Load the model for conversion\n",
    "loaded_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "# Create TFLite converteconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "# Create TFLite converter with INT4 quantization\n",
    "# Enhanced quantization configuration\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Set target spec for INT4\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "]\n",
    "\n",
    "# Increase calibration samples\n",
    "def representative_data_gen():\n",
    "    num_calibration_samples = 1000  # Increased from 100\n",
    "    for i in range(num_calibration_samples):\n",
    "        # Add noise for robustness\n",
    "        image = x_train_processed[i:i+1].astype(np.float32)\n",
    "        image += np.random.normal(0, 0.01, image.shape)\n",
    "        image = np.clip(image, 0, 1)\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "\n",
    "# Force 4-bit weights through post-training quantization\n",
    "converter.target_spec._experimental_low_bit_qat = True\n",
    "converter.target_spec._experimental_weight_bits = 4\n",
    "\n",
    "# Convert model to TFLite format\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "with open(tflite_model_quant_filename, \"wb\") as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "print(f\"\\nInt4 quantized TFLite model saved to {tflite_model_quant_filename}\")\n",
    "\n",
    "# Check model size\n",
    "tflite_model_quant_size_bytes = os.path.getsize(tflite_model_quant_filename)\n",
    "tflite_model_quant_size_kb = tflite_model_quant_size_bytes / 1024\n",
    "print(f\"Int4 Quantized Model Size: {tflite_model_quant_size_bytes} bytes ({tflite_model_quant_size_kb:.2f} KB)\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(keras_model_path)\n",
    "print(f\"Cleaned up temporary Keras model file: {keras_model_path}\")\n",
    "\n",
    "# Create an interpreter to analyze the model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_quant_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nModel Details:\")\n",
    "print(\"Input:\", input_details)\n",
    "print(\"Output:\", output_details)\n",
    "\n",
    "# Print quantization parameters\n",
    "print(\"\\nQuantization Parameters:\")\n",
    "print(f\"Input Scale: {input_details[0]['quantization_parameters']['scales']}\")\n",
    "print(f\"Input Zero Point: {input_details[0]['quantization_parameters']['zero_points']}\")\n",
    "print(f\"Output Scale: {output_details[0]['quantization_parameters']['scales']}\")\n",
    "print(f\"Output Zero Point: {output_details[0]['quantization_parameters']['zero_points']}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "18c49830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keras model saved to mnist_baseline_model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpi6ipg8iw\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmpi6ipg8iw\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INT8 TFLite model saved to mnist_model_int8.tflite\n",
      "INT8 Model Size: 17768 bytes (17.35 KB)\n",
      "Cleaned up temporary Keras model file: mnist_baseline_model.keras\n",
      "\n",
      "Model Details:\n",
      "Input: [{'name': 'serving_default_flatten_12_input:0', 'index': 0, 'shape': array([ 1, 12, 12,  1]), 'shape_signature': array([-1, 12, 12,  1]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003921568859368563, -128), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output: [{'name': 'StatefulPartitionedCall:0', 'index': 14, 'shape': array([ 1, 10]), 'shape_signature': array([-1, 10]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the model in Keras format\n",
    "keras_model_path = \"mnist_baseline_model.keras\"\n",
    "model.save(keras_model_path)\n",
    "print(f\"\\nKeras model saved to {keras_model_path}\")\n",
    "\n",
    "# Load the model for conversion\n",
    "loaded_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "# Create TFLite converter with INT8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Configure for INT8\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS\n",
    "]\n",
    "\n",
    "# Representative dataset for calibration\n",
    "def representative_data_gen():\n",
    "    num_calibration_samples = 100\n",
    "    for i in range(num_calibration_samples):\n",
    "        image = x_train_processed[i:i+1].astype(np.float32)\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert model to TFLite format\n",
    "tflite_model_int8 = converter.convert()\n",
    "\n",
    "# Save the INT8 quantized model\n",
    "tflite_model_filename = \"mnist_model_int8.tflite\"\n",
    "with open(tflite_model_filename, \"wb\") as f:\n",
    "    f.write(tflite_model_int8)\n",
    "\n",
    "print(f\"\\nINT8 TFLite model saved to {tflite_model_filename}\")\n",
    "\n",
    "# Check model size\n",
    "model_size_bytes = os.path.getsize(tflite_model_filename)\n",
    "model_size_kb = model_size_bytes / 1024\n",
    "print(f\"INT8 Model Size: {model_size_bytes} bytes ({model_size_kb:.2f} KB)\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(keras_model_path)\n",
    "print(f\"Cleaned up temporary Keras model file: {keras_model_path}\")\n",
    "\n",
    "# Create an interpreter to analyze the model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nModel Details:\")\n",
    "print(\"Input:\", input_details)\n",
    "print(\"Output:\", output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f1aa5ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Generate a .h file for the quantized TFLite model\\n\\nimport os\\n\\ntflite_model_quant_filename = \"mnist_quantized_model.tflite\"\\nc_model_filename = \"mnist_model_data.h\"\\n\\n# Read the TFLite model file as bytes\\nwith open(tflite_model_quant_filename, \"rb\") as f:\\n    tflite_model_bytes = f.read()\\n\\n# Convert bytes to a C-style array declaration\\n# Format: const unsigned char model_name[] = { 0x.., 0x.., ... };\\nbytes_as_hex = [f\"0x{byte:02x}\" for byte in tflite_model_bytes]\\nc_array_declaration = f\"const unsigned char {os.path.basename(tflite_model_quant_filename).replace(\\'.\\', \\'_\\')}_data[] = {{\\n    \"\\nc_array_declaration += \", \".join(bytes_as_hex)\\nc_array_declaration += \"\\n};\\n\"\\nc_array_declaration += f\"const int {os.path.basename(tflite_model_quant_filename).replace(\\'.\\', \\'_\\')}_len = {len(tflite_model_bytes)};\\n\"\\n\\n# Write the C array declaration to the .h file\\nwith open(c_model_filename, \"w\") as f:\\n    f.write(\"#ifndef MNIST_MODEL_DATA_H_\\n\")\\n    f.write(\"#define MNIST_MODEL_DATA_H_\\n\\n\")\\n    f.write(c_array_declaration)\\n    f.write(\"\\n#endif // MNIST_MODEL_DATA_H_\\n\")\\n\\nprint(f\"\\nC header file generated: {c_model_filename}\")\\nprint(f\"This file contains the quantized model as a byte array, ready for embedded C inclusion.\")\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Generate a .h file for the quantized TFLite model\n",
    "\n",
    "import os\n",
    "\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "c_model_filename = \"mnist_model_data.h\"\n",
    "\n",
    "# Read the TFLite model file as bytes\n",
    "with open(tflite_model_quant_filename, \"rb\") as f:\n",
    "    tflite_model_bytes = f.read()\n",
    "\n",
    "# Convert bytes to a C-style array declaration\n",
    "# Format: const unsigned char model_name[] = { 0x.., 0x.., ... };\n",
    "bytes_as_hex = [f\"0x{byte:02x}\" for byte in tflite_model_bytes]\n",
    "c_array_declaration = f\"const unsigned char {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_data[] = {{\\n    \"\n",
    "c_array_declaration += \", \".join(bytes_as_hex)\n",
    "c_array_declaration += \"\\n};\\n\"\n",
    "c_array_declaration += f\"const int {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_len = {len(tflite_model_bytes)};\\n\"\n",
    "\n",
    "# Write the C array declaration to the .h file\n",
    "with open(c_model_filename, \"w\") as f:\n",
    "    f.write(\"#ifndef MNIST_MODEL_DATA_H_\\n\")\n",
    "    f.write(\"#define MNIST_MODEL_DATA_H_\\n\\n\")\n",
    "    f.write(c_array_declaration)\n",
    "    f.write(\"\\n#endif // MNIST_MODEL_DATA_H_\\n\")\n",
    "\n",
    "print(f\"\\nC header file generated: {c_model_filename}\")\n",
    "print(f\"This file contains the quantized model as a byte array, ready for embedded C inclusion.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "54af4c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C header file generated: mnist_model_data.h\n",
      "Model size: 17768 bytes (17.35 KB)\n",
      "Header file contains INT8 quantized model as a byte array with:\n",
      "- Improved formatting (16 elements per line)\n",
      "- Added metadata comments\n",
      "- INT8 specific naming\n",
      "- stdint.h inclusion for fixed-width types\n"
     ]
    }
   ],
   "source": [
    "# Generate a .h file for the INT8 TFLite model\n",
    "import os\n",
    "\n",
    "# Update filename to match the INT8 model\n",
    "tflite_model_filename = \"mnist_model_int8.tflite\"\n",
    "c_model_filename = \"mnist_model_data.h\"\n",
    "\n",
    "# Read the TFLite model file as bytes\n",
    "with open(tflite_model_filename, \"rb\") as f:\n",
    "    tflite_model_bytes = f.read()\n",
    "\n",
    "# Convert bytes to a C-style array declaration with line breaks every 16 elements\n",
    "bytes_as_hex = [f\"0x{byte:02x}\" for byte in tflite_model_bytes]\n",
    "elements_per_line = 16\n",
    "lines = []\n",
    "for i in range(0, len(bytes_as_hex), elements_per_line):\n",
    "    line = bytes_as_hex[i:i + elements_per_line]\n",
    "    lines.append(\"    \" + \", \".join(line))\n",
    "\n",
    "c_array_declaration = f\"const unsigned char mnist_model_int8_data[] = {{\\n\"\n",
    "c_array_declaration += \",\\n\".join(lines)\n",
    "c_array_declaration += \"\\n};\\n\\n\"\n",
    "c_array_declaration += f\"const int mnist_model_int8_len = {len(tflite_model_bytes)};\\n\"\n",
    "\n",
    "# Write the C header file with additional metadata\n",
    "with open(c_model_filename, \"w\") as f:\n",
    "    f.write(\"/*\\n\")\n",
    "    f.write(\" * MNIST Model Data - INT8 Quantized Version\\n\")\n",
    "    f.write(\" * Auto-generated header file for SiFive HiFive1 Board\\n\")\n",
    "    f.write(\" * Model: 8x8 MNIST Classifier\\n\")\n",
    "    f.write(\" * Quantization: INT8\\n\")\n",
    "    f.write(\" */\\n\\n\")\n",
    "    f.write(\"#ifndef MNIST_MODEL_DATA_H_\\n\")\n",
    "    f.write(\"#define MNIST_MODEL_DATA_H_\\n\\n\")\n",
    "    f.write(\"#include <stdint.h>\\n\\n\")\n",
    "    f.write(c_array_declaration)\n",
    "    f.write(\"\\n#endif // MNIST_MODEL_DATA_H_\\n\")\n",
    "\n",
    "print(f\"\\nC header file generated: {c_model_filename}\")\n",
    "print(f\"Model size: {len(tflite_model_bytes)} bytes ({len(tflite_model_bytes)/1024:.2f} KB)\")\n",
    "print(\"Header file contains INT8 quantized model as a byte array with:\")\n",
    "print(\"- Improved formatting (16 elements per line)\")\n",
    "print(\"- Added metadata comments\")\n",
    "print(\"- INT8 specific naming\")\n",
    "print(\"- stdint.h inclusion for fixed-width types\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
