{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6f001ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (60000, 28, 28)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of x_test: (10000, 28, 28)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f331a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train after reshaping: (60000, 28, 28, 1)\n",
      "Shape of x_test after reshaping: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Reshape images\n",
    "# Images are 28x28 grayscale, so we add a channel dimension of 1\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"Shape of x_train after reshaping: {x_train.shape}\")\n",
    "print(f\"Shape of x_test after reshaping: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75feeec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min pixel value in x_train after normalization: 0.0\n",
      "Max pixel value in x_train after normalization: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Normalize pixel values\n",
    "# Convert pixel values from [0, 255] to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "print(f\"Min pixel value in x_train after normalization: {x_train.min()}\")\n",
    "print(f\"Max pixel value in x_train after normalization: {x_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b6af7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train after one-hot encoding: (60000, 10)\n",
      "Shape of y_test after one-hot encoding: (10000, 10)\n",
      "Example of one-hot encoded label (first training label): [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: One-hot encode labels\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(f\"Shape of y_train after one-hot encoding: {y_train.shape}\")\n",
    "print(f\"Shape of y_test after one-hot encoding: {y_test.shape}\")\n",
    "print(f\"Example of one-hot encoded label (first training label): {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f94261c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25818 (100.85 KB)\n",
      "Trainable params: 25818 (100.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25818 (100.85 KB)\n",
      "Trainable params: 25818 (100.85 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Network Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Define optimized model for SiFive HiFive1\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28, 1)),\n",
    "    # Increase neurons while staying within memory constraints\n",
    "    Dense(32, activation='relu',\n",
    "          kernel_regularizer=l1(0.005),  # Reduced L1 penalty\n",
    "          kernel_constraint=tf.keras.constraints.MaxNorm(3)), # Add max norm constraint\n",
    "    Dense(16, activation='relu',\n",
    "          kernel_regularizer=l1(0.005)),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d16e78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\n"
     ]
    }
   ],
   "source": [
    "# Improved training configuration\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully with Adam optimizer, Categorical Crossentropy loss, and Accuracy metric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b88313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "469/469 [==============================] - 2s 2ms/step - loss: 2.0179 - accuracy: 0.7815 - val_loss: 1.2138 - val_accuracy: 0.8649 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "469/469 [==============================] - 2s 2ms/step - loss: 2.0179 - accuracy: 0.7815 - val_loss: 1.2138 - val_accuracy: 0.8649 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1274 - accuracy: 0.8686 - val_loss: 1.0220 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1274 - accuracy: 0.8686 - val_loss: 1.0220 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.9924 - accuracy: 0.8788 - val_loss: 0.9446 - val_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.9924 - accuracy: 0.8788 - val_loss: 0.9446 - val_accuracy: 0.8800 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.9175 - accuracy: 0.8828 - val_loss: 0.8704 - val_accuracy: 0.8880 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.9175 - accuracy: 0.8828 - val_loss: 0.8704 - val_accuracy: 0.8880 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8642 - accuracy: 0.8854 - val_loss: 0.8236 - val_accuracy: 0.8916 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8642 - accuracy: 0.8854 - val_loss: 0.8236 - val_accuracy: 0.8916 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8235 - accuracy: 0.8885 - val_loss: 0.7991 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8235 - accuracy: 0.8885 - val_loss: 0.7991 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7942 - accuracy: 0.8903 - val_loss: 0.7645 - val_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7942 - accuracy: 0.8903 - val_loss: 0.7645 - val_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7721 - accuracy: 0.8912 - val_loss: 0.7462 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7721 - accuracy: 0.8912 - val_loss: 0.7462 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7546 - accuracy: 0.8931 - val_loss: 0.7355 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7546 - accuracy: 0.8931 - val_loss: 0.7355 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7443 - accuracy: 0.8936 - val_loss: 0.7195 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7443 - accuracy: 0.8936 - val_loss: 0.7195 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7328 - accuracy: 0.8955 - val_loss: 0.7124 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7328 - accuracy: 0.8955 - val_loss: 0.7124 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7245 - accuracy: 0.8964 - val_loss: 0.7042 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7245 - accuracy: 0.8964 - val_loss: 0.7042 - val_accuracy: 0.9026 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7155 - accuracy: 0.8963 - val_loss: 0.7085 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7155 - accuracy: 0.8963 - val_loss: 0.7085 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7073 - accuracy: 0.8962 - val_loss: 0.6889 - val_accuracy: 0.9008 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.7073 - accuracy: 0.8962 - val_loss: 0.6889 - val_accuracy: 0.9008 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7002 - accuracy: 0.8973 - val_loss: 0.6755 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.7002 - accuracy: 0.8973 - val_loss: 0.6755 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.8977 - val_loss: 0.6844 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6957 - accuracy: 0.8977 - val_loss: 0.6844 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6850 - accuracy: 0.8998 - val_loss: 0.6758 - val_accuracy: 0.9002 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6850 - accuracy: 0.8998 - val_loss: 0.6758 - val_accuracy: 0.9002 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.8989 - val_loss: 0.6605 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.8989 - val_loss: 0.6605 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6768 - accuracy: 0.8996 - val_loss: 0.6606 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6768 - accuracy: 0.8996 - val_loss: 0.6606 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6692 - accuracy: 0.9003 - val_loss: 0.6653 - val_accuracy: 0.8966 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6692 - accuracy: 0.9003 - val_loss: 0.6653 - val_accuracy: 0.8966 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6628 - accuracy: 0.9004 - val_loss: 0.6418 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6628 - accuracy: 0.9004 - val_loss: 0.6418 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6555 - accuracy: 0.9025 - val_loss: 0.6458 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6555 - accuracy: 0.9025 - val_loss: 0.6458 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.9027 - val_loss: 0.6409 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.9027 - val_loss: 0.6409 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.9029 - val_loss: 0.6399 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.9029 - val_loss: 0.6399 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.9026 - val_loss: 0.6485 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.9026 - val_loss: 0.6485 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6455 - accuracy: 0.9022 - val_loss: 0.6598 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6455 - accuracy: 0.9022 - val_loss: 0.6598 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6369 - accuracy: 0.9048 - val_loss: 0.6326 - val_accuracy: 0.9044 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6369 - accuracy: 0.9048 - val_loss: 0.6326 - val_accuracy: 0.9044 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.9038 - val_loss: 0.6159 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.9038 - val_loss: 0.6159 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6353 - accuracy: 0.9043 - val_loss: 0.6181 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6353 - accuracy: 0.9043 - val_loss: 0.6181 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.9049 - val_loss: 0.6145 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.9049 - val_loss: 0.6145 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.9065 - val_loss: 0.6130 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.9065 - val_loss: 0.6130 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.9058 - val_loss: 0.6104 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.9058 - val_loss: 0.6104 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.9056 - val_loss: 0.6076 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.9056 - val_loss: 0.6076 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.9060 - val_loss: 0.6052 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.9060 - val_loss: 0.6052 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.9055 - val_loss: 0.6013 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6184 - accuracy: 0.9055 - val_loss: 0.6013 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.9064 - val_loss: 0.6007 - val_accuracy: 0.9094 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.9064 - val_loss: 0.6007 - val_accuracy: 0.9094 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6114 - accuracy: 0.9073 - val_loss: 0.6085 - val_accuracy: 0.9092 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6114 - accuracy: 0.9073 - val_loss: 0.6085 - val_accuracy: 0.9092 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.9075 - val_loss: 0.6005 - val_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.9075 - val_loss: 0.6005 - val_accuracy: 0.9102 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6089 - accuracy: 0.9063 - val_loss: 0.5876 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6089 - accuracy: 0.9063 - val_loss: 0.5876 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6056 - accuracy: 0.9076 - val_loss: 0.5998 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6056 - accuracy: 0.9076 - val_loss: 0.5998 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6047 - accuracy: 0.9071 - val_loss: 0.6113 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6047 - accuracy: 0.9071 - val_loss: 0.6113 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5982 - accuracy: 0.9084 - val_loss: 0.5928 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5982 - accuracy: 0.9084 - val_loss: 0.5928 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5977 - accuracy: 0.9088 - val_loss: 0.5927 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5977 - accuracy: 0.9088 - val_loss: 0.5927 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5935 - accuracy: 0.9094 - val_loss: 0.5835 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5935 - accuracy: 0.9094 - val_loss: 0.5835 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5937 - accuracy: 0.9089 - val_loss: 0.5921 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5937 - accuracy: 0.9089 - val_loss: 0.5921 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5933 - accuracy: 0.9080 - val_loss: 0.5849 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5933 - accuracy: 0.9080 - val_loss: 0.5849 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5876 - accuracy: 0.9108 - val_loss: 0.5723 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5876 - accuracy: 0.9108 - val_loss: 0.5723 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5887 - accuracy: 0.9093 - val_loss: 0.5809 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5887 - accuracy: 0.9093 - val_loss: 0.5809 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.9095 - val_loss: 0.5866 - val_accuracy: 0.9038 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.9095 - val_loss: 0.5866 - val_accuracy: 0.9038 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5869 - accuracy: 0.9081 - val_loss: 0.5667 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5869 - accuracy: 0.9081 - val_loss: 0.5667 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5847 - accuracy: 0.9099 - val_loss: 0.5745 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5847 - accuracy: 0.9099 - val_loss: 0.5745 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5842 - accuracy: 0.9106 - val_loss: 0.5734 - val_accuracy: 0.9096 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5842 - accuracy: 0.9106 - val_loss: 0.5734 - val_accuracy: 0.9096 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5806 - accuracy: 0.9100 - val_loss: 0.5777 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5806 - accuracy: 0.9100 - val_loss: 0.5777 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5790 - accuracy: 0.9112 - val_loss: 0.5780 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5790 - accuracy: 0.9112 - val_loss: 0.5780 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5767 - accuracy: 0.9107 - val_loss: 0.5675 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5767 - accuracy: 0.9107 - val_loss: 0.5675 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5464 - accuracy: 0.9160 - val_loss: 0.5381 - val_accuracy: 0.9133 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5464 - accuracy: 0.9160 - val_loss: 0.5381 - val_accuracy: 0.9133 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.9160 - val_loss: 0.5289 - val_accuracy: 0.9196 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.9160 - val_loss: 0.5289 - val_accuracy: 0.9196 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5417 - accuracy: 0.9162 - val_loss: 0.5432 - val_accuracy: 0.9113 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5417 - accuracy: 0.9162 - val_loss: 0.5432 - val_accuracy: 0.9113 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5426 - accuracy: 0.9154 - val_loss: 0.5423 - val_accuracy: 0.9153 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5426 - accuracy: 0.9154 - val_loss: 0.5423 - val_accuracy: 0.9153 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5428 - accuracy: 0.9154 - val_loss: 0.5368 - val_accuracy: 0.9161 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5428 - accuracy: 0.9154 - val_loss: 0.5368 - val_accuracy: 0.9161 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.9157 - val_loss: 0.5327 - val_accuracy: 0.9141 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.9157 - val_loss: 0.5327 - val_accuracy: 0.9141 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.9155 - val_loss: 0.5363 - val_accuracy: 0.9178 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.9155 - val_loss: 0.5363 - val_accuracy: 0.9178 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5227 - accuracy: 0.9183 - val_loss: 0.5213 - val_accuracy: 0.9146 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5227 - accuracy: 0.9183 - val_loss: 0.5213 - val_accuracy: 0.9146 - lr: 2.5000e-04\n",
      "Epoch 64/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5220 - accuracy: 0.9175 - val_loss: 0.5132 - val_accuracy: 0.9173 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5220 - accuracy: 0.9175 - val_loss: 0.5132 - val_accuracy: 0.9173 - lr: 2.5000e-04\n",
      "Epoch 65/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5204 - accuracy: 0.9189 - val_loss: 0.5161 - val_accuracy: 0.9174 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5204 - accuracy: 0.9189 - val_loss: 0.5161 - val_accuracy: 0.9174 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5202 - accuracy: 0.9185 - val_loss: 0.5137 - val_accuracy: 0.9181 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5202 - accuracy: 0.9185 - val_loss: 0.5137 - val_accuracy: 0.9181 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5196 - accuracy: 0.9181 - val_loss: 0.5150 - val_accuracy: 0.9176 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5196 - accuracy: 0.9181 - val_loss: 0.5150 - val_accuracy: 0.9176 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5197 - accuracy: 0.9183 - val_loss: 0.5082 - val_accuracy: 0.9208 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5197 - accuracy: 0.9183 - val_loss: 0.5082 - val_accuracy: 0.9208 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5192 - accuracy: 0.9185 - val_loss: 0.5139 - val_accuracy: 0.9179 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5192 - accuracy: 0.9185 - val_loss: 0.5139 - val_accuracy: 0.9179 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5182 - accuracy: 0.9183 - val_loss: 0.5114 - val_accuracy: 0.9194 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5182 - accuracy: 0.9183 - val_loss: 0.5114 - val_accuracy: 0.9194 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5178 - accuracy: 0.9189 - val_loss: 0.5122 - val_accuracy: 0.9169 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5178 - accuracy: 0.9189 - val_loss: 0.5122 - val_accuracy: 0.9169 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5168 - accuracy: 0.9190 - val_loss: 0.5164 - val_accuracy: 0.9147 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5168 - accuracy: 0.9190 - val_loss: 0.5164 - val_accuracy: 0.9147 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5168 - accuracy: 0.9189 - val_loss: 0.5074 - val_accuracy: 0.9203 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5168 - accuracy: 0.9189 - val_loss: 0.5074 - val_accuracy: 0.9203 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5159 - accuracy: 0.9190 - val_loss: 0.5074 - val_accuracy: 0.9186 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5159 - accuracy: 0.9190 - val_loss: 0.5074 - val_accuracy: 0.9186 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.9189 - val_loss: 0.5119 - val_accuracy: 0.9153 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.9189 - val_loss: 0.5119 - val_accuracy: 0.9153 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.9194 - val_loss: 0.5058 - val_accuracy: 0.9201 - lr: 2.5000e-04\n",
      "Epoch 77/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5155 - accuracy: 0.9194 - val_loss: 0.5058 - val_accuracy: 0.9201 - lr: 2.5000e-04\n",
      "Epoch 77/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5153 - accuracy: 0.9187 - val_loss: 0.5082 - val_accuracy: 0.9179 - lr: 2.5000e-04\n",
      "Epoch 78/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5153 - accuracy: 0.9187 - val_loss: 0.5082 - val_accuracy: 0.9179 - lr: 2.5000e-04\n",
      "Epoch 78/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5143 - accuracy: 0.9193 - val_loss: 0.5039 - val_accuracy: 0.9209 - lr: 2.5000e-04\n",
      "Epoch 79/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5143 - accuracy: 0.9193 - val_loss: 0.5039 - val_accuracy: 0.9209 - lr: 2.5000e-04\n",
      "Epoch 79/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5133 - accuracy: 0.9187 - val_loss: 0.5084 - val_accuracy: 0.9190 - lr: 2.5000e-04\n",
      "Epoch 80/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5133 - accuracy: 0.9187 - val_loss: 0.5084 - val_accuracy: 0.9190 - lr: 2.5000e-04\n",
      "Epoch 80/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5142 - accuracy: 0.9191 - val_loss: 0.5054 - val_accuracy: 0.9194 - lr: 2.5000e-04\n",
      "Epoch 81/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5142 - accuracy: 0.9191 - val_loss: 0.5054 - val_accuracy: 0.9194 - lr: 2.5000e-04\n",
      "Epoch 81/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5134 - accuracy: 0.9190 - val_loss: 0.5105 - val_accuracy: 0.9172 - lr: 2.5000e-04\n",
      "Epoch 82/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5134 - accuracy: 0.9190 - val_loss: 0.5105 - val_accuracy: 0.9172 - lr: 2.5000e-04\n",
      "Epoch 82/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5127 - accuracy: 0.9201 - val_loss: 0.5064 - val_accuracy: 0.9190 - lr: 2.5000e-04\n",
      "Epoch 83/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5127 - accuracy: 0.9201 - val_loss: 0.5064 - val_accuracy: 0.9190 - lr: 2.5000e-04\n",
      "Epoch 83/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5118 - accuracy: 0.9194 - val_loss: 0.5092 - val_accuracy: 0.9165 - lr: 2.5000e-04\n",
      "Epoch 84/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5118 - accuracy: 0.9194 - val_loss: 0.5092 - val_accuracy: 0.9165 - lr: 2.5000e-04\n",
      "Epoch 84/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5026 - accuracy: 0.9207 - val_loss: 0.4995 - val_accuracy: 0.9195 - lr: 1.2500e-04\n",
      "Epoch 85/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5026 - accuracy: 0.9207 - val_loss: 0.4995 - val_accuracy: 0.9195 - lr: 1.2500e-04\n",
      "Epoch 85/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5015 - accuracy: 0.9207 - val_loss: 0.4976 - val_accuracy: 0.9203 - lr: 1.2500e-04\n",
      "Epoch 86/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5015 - accuracy: 0.9207 - val_loss: 0.4976 - val_accuracy: 0.9203 - lr: 1.2500e-04\n",
      "Epoch 86/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5018 - accuracy: 0.9208 - val_loss: 0.4958 - val_accuracy: 0.9200 - lr: 1.2500e-04\n",
      "Epoch 87/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5018 - accuracy: 0.9208 - val_loss: 0.4958 - val_accuracy: 0.9200 - lr: 1.2500e-04\n",
      "Epoch 87/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5011 - accuracy: 0.9203 - val_loss: 0.4960 - val_accuracy: 0.9193 - lr: 1.2500e-04\n",
      "Epoch 88/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5011 - accuracy: 0.9203 - val_loss: 0.4960 - val_accuracy: 0.9193 - lr: 1.2500e-04\n",
      "Epoch 88/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5005 - accuracy: 0.9214 - val_loss: 0.4949 - val_accuracy: 0.9198 - lr: 1.2500e-04\n",
      "Epoch 89/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5005 - accuracy: 0.9214 - val_loss: 0.4949 - val_accuracy: 0.9198 - lr: 1.2500e-04\n",
      "Epoch 89/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.9204 - val_loss: 0.4968 - val_accuracy: 0.9199 - lr: 1.2500e-04\n",
      "Epoch 90/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.9204 - val_loss: 0.4968 - val_accuracy: 0.9199 - lr: 1.2500e-04\n",
      "Epoch 90/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5010 - accuracy: 0.9209 - val_loss: 0.4948 - val_accuracy: 0.9212 - lr: 1.2500e-04\n",
      "Epoch 91/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5010 - accuracy: 0.9209 - val_loss: 0.4948 - val_accuracy: 0.9212 - lr: 1.2500e-04\n",
      "Epoch 91/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5000 - accuracy: 0.9215 - val_loss: 0.4939 - val_accuracy: 0.9208 - lr: 1.2500e-04\n",
      "Epoch 92/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.5000 - accuracy: 0.9215 - val_loss: 0.4939 - val_accuracy: 0.9208 - lr: 1.2500e-04\n",
      "Epoch 92/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4999 - accuracy: 0.9215 - val_loss: 0.4988 - val_accuracy: 0.9187 - lr: 1.2500e-04\n",
      "Epoch 93/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4999 - accuracy: 0.9215 - val_loss: 0.4988 - val_accuracy: 0.9187 - lr: 1.2500e-04\n",
      "Epoch 93/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.9215 - val_loss: 0.4939 - val_accuracy: 0.9207 - lr: 1.2500e-04\n",
      "Epoch 94/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.9215 - val_loss: 0.4939 - val_accuracy: 0.9207 - lr: 1.2500e-04\n",
      "Epoch 94/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4996 - accuracy: 0.9213 - val_loss: 0.4955 - val_accuracy: 0.9211 - lr: 1.2500e-04\n",
      "Epoch 95/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4996 - accuracy: 0.9213 - val_loss: 0.4955 - val_accuracy: 0.9211 - lr: 1.2500e-04\n",
      "Epoch 95/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4985 - accuracy: 0.9218 - val_loss: 0.4961 - val_accuracy: 0.9192 - lr: 1.2500e-04\n",
      "Epoch 96/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4985 - accuracy: 0.9218 - val_loss: 0.4961 - val_accuracy: 0.9192 - lr: 1.2500e-04\n",
      "Epoch 96/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4990 - accuracy: 0.9210 - val_loss: 0.4948 - val_accuracy: 0.9192 - lr: 1.2500e-04\n",
      "Epoch 97/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4990 - accuracy: 0.9210 - val_loss: 0.4948 - val_accuracy: 0.9192 - lr: 1.2500e-04\n",
      "Epoch 97/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4973 - accuracy: 0.9219 - val_loss: 0.4943 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4973 - accuracy: 0.9219 - val_loss: 0.4943 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Epoch 98/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.9216 - val_loss: 0.4911 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4962 - accuracy: 0.9216 - val_loss: 0.4911 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 99/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.9220 - val_loss: 0.4913 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.9220 - val_loss: 0.4913 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 100/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.9215 - val_loss: 0.4916 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.9215 - val_loss: 0.4916 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 101/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4962 - accuracy: 0.9210 - val_loss: 0.4917 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4962 - accuracy: 0.9210 - val_loss: 0.4917 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 102/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4958 - accuracy: 0.9219 - val_loss: 0.4901 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4958 - accuracy: 0.9219 - val_loss: 0.4901 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 103/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4954 - accuracy: 0.9215 - val_loss: 0.4915 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4954 - accuracy: 0.9215 - val_loss: 0.4915 - val_accuracy: 0.9205 - lr: 1.0000e-04\n",
      "Epoch 104/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4952 - accuracy: 0.9222 - val_loss: 0.4923 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4952 - accuracy: 0.9222 - val_loss: 0.4923 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 105/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4952 - accuracy: 0.9215 - val_loss: 0.4922 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4952 - accuracy: 0.9215 - val_loss: 0.4922 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 106/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4945 - accuracy: 0.9222 - val_loss: 0.4905 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4945 - accuracy: 0.9222 - val_loss: 0.4905 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 107/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4946 - accuracy: 0.9219 - val_loss: 0.4930 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4946 - accuracy: 0.9219 - val_loss: 0.4930 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Epoch 108/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.9214 - val_loss: 0.4923 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.9214 - val_loss: 0.4923 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Epoch 109/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4943 - accuracy: 0.9222 - val_loss: 0.4884 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4943 - accuracy: 0.9222 - val_loss: 0.4884 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 110/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.9226 - val_loss: 0.4906 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4944 - accuracy: 0.9226 - val_loss: 0.4906 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 111/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4937 - accuracy: 0.9220 - val_loss: 0.4917 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4937 - accuracy: 0.9220 - val_loss: 0.4917 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 112/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4936 - accuracy: 0.9226 - val_loss: 0.4911 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4936 - accuracy: 0.9226 - val_loss: 0.4911 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 113/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.9220 - val_loss: 0.4893 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.9220 - val_loss: 0.4893 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 114/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.9218 - val_loss: 0.4907 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.9218 - val_loss: 0.4907 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 115/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4928 - accuracy: 0.9221 - val_loss: 0.4915 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4928 - accuracy: 0.9221 - val_loss: 0.4915 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Epoch 116/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4932 - accuracy: 0.9225 - val_loss: 0.4881 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4932 - accuracy: 0.9225 - val_loss: 0.4881 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 117/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4934 - accuracy: 0.9221 - val_loss: 0.4884 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4934 - accuracy: 0.9221 - val_loss: 0.4884 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 118/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4927 - accuracy: 0.9225 - val_loss: 0.4892 - val_accuracy: 0.9193 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4927 - accuracy: 0.9225 - val_loss: 0.4892 - val_accuracy: 0.9193 - lr: 1.0000e-04\n",
      "Epoch 119/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4922 - accuracy: 0.9223 - val_loss: 0.4883 - val_accuracy: 0.9191 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4922 - accuracy: 0.9223 - val_loss: 0.4883 - val_accuracy: 0.9191 - lr: 1.0000e-04\n",
      "Epoch 120/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4921 - accuracy: 0.9221 - val_loss: 0.4916 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4921 - accuracy: 0.9221 - val_loss: 0.4916 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 121/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.9228 - val_loss: 0.4922 - val_accuracy: 0.9187 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4918 - accuracy: 0.9228 - val_loss: 0.4922 - val_accuracy: 0.9187 - lr: 1.0000e-04\n",
      "Epoch 122/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4920 - accuracy: 0.9222 - val_loss: 0.4895 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4920 - accuracy: 0.9222 - val_loss: 0.4895 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 123/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4919 - accuracy: 0.9223 - val_loss: 0.4859 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4919 - accuracy: 0.9223 - val_loss: 0.4859 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 124/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.9218 - val_loss: 0.4883 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.9218 - val_loss: 0.4883 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 125/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.9223 - val_loss: 0.4887 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4914 - accuracy: 0.9223 - val_loss: 0.4887 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 126/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4917 - accuracy: 0.9222 - val_loss: 0.4899 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4917 - accuracy: 0.9222 - val_loss: 0.4899 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 127/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4913 - accuracy: 0.9216 - val_loss: 0.4863 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4913 - accuracy: 0.9216 - val_loss: 0.4863 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 128/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4908 - accuracy: 0.9233 - val_loss: 0.4866 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4908 - accuracy: 0.9233 - val_loss: 0.4866 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 129/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4907 - accuracy: 0.9225 - val_loss: 0.4857 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4907 - accuracy: 0.9225 - val_loss: 0.4857 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 130/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4905 - accuracy: 0.9224 - val_loss: 0.4907 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4905 - accuracy: 0.9224 - val_loss: 0.4907 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 131/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4904 - accuracy: 0.9226 - val_loss: 0.4860 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4904 - accuracy: 0.9226 - val_loss: 0.4860 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 132/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.9222 - val_loss: 0.4877 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4901 - accuracy: 0.9222 - val_loss: 0.4877 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 133/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4900 - accuracy: 0.9220 - val_loss: 0.4853 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4900 - accuracy: 0.9220 - val_loss: 0.4853 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 134/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4900 - accuracy: 0.9225 - val_loss: 0.4848 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4900 - accuracy: 0.9225 - val_loss: 0.4848 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 135/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4892 - accuracy: 0.9232 - val_loss: 0.4855 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4892 - accuracy: 0.9232 - val_loss: 0.4855 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 136/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4891 - accuracy: 0.9232 - val_loss: 0.4879 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4891 - accuracy: 0.9232 - val_loss: 0.4879 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 137/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4897 - accuracy: 0.9217 - val_loss: 0.4871 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4897 - accuracy: 0.9217 - val_loss: 0.4871 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 138/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4892 - accuracy: 0.9230 - val_loss: 0.4857 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4892 - accuracy: 0.9230 - val_loss: 0.4857 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 139/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4886 - accuracy: 0.9232 - val_loss: 0.4861 - val_accuracy: 0.9187 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4886 - accuracy: 0.9232 - val_loss: 0.4861 - val_accuracy: 0.9187 - lr: 1.0000e-04\n",
      "Epoch 140/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4889 - accuracy: 0.9222 - val_loss: 0.4858 - val_accuracy: 0.9201 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4889 - accuracy: 0.9222 - val_loss: 0.4858 - val_accuracy: 0.9201 - lr: 1.0000e-04\n",
      "Epoch 141/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4887 - accuracy: 0.9225 - val_loss: 0.4871 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4887 - accuracy: 0.9225 - val_loss: 0.4871 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 142/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.9228 - val_loss: 0.4838 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.9228 - val_loss: 0.4838 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 143/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4882 - accuracy: 0.9226 - val_loss: 0.4853 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4882 - accuracy: 0.9226 - val_loss: 0.4853 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 144/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4886 - accuracy: 0.9236 - val_loss: 0.4874 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4886 - accuracy: 0.9236 - val_loss: 0.4874 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 145/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.9229 - val_loss: 0.4874 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.9229 - val_loss: 0.4874 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 146/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.9221 - val_loss: 0.4846 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4888 - accuracy: 0.9221 - val_loss: 0.4846 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 147/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.9227 - val_loss: 0.4839 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.9227 - val_loss: 0.4839 - val_accuracy: 0.9207 - lr: 1.0000e-04\n",
      "Epoch 148/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4876 - accuracy: 0.9229 - val_loss: 0.4844 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4876 - accuracy: 0.9229 - val_loss: 0.4844 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 149/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4873 - accuracy: 0.9236 - val_loss: 0.4854 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4873 - accuracy: 0.9236 - val_loss: 0.4854 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 150/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4877 - accuracy: 0.9232 - val_loss: 0.4881 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4877 - accuracy: 0.9232 - val_loss: 0.4881 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Epoch 151/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4873 - accuracy: 0.9235 - val_loss: 0.4829 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4873 - accuracy: 0.9235 - val_loss: 0.4829 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 152/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4867 - accuracy: 0.9227 - val_loss: 0.4841 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4867 - accuracy: 0.9227 - val_loss: 0.4841 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 153/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4868 - accuracy: 0.9230 - val_loss: 0.4851 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4868 - accuracy: 0.9230 - val_loss: 0.4851 - val_accuracy: 0.9189 - lr: 1.0000e-04\n",
      "Epoch 154/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4868 - accuracy: 0.9239 - val_loss: 0.4821 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4868 - accuracy: 0.9239 - val_loss: 0.4821 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 155/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4864 - accuracy: 0.9233 - val_loss: 0.4823 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4864 - accuracy: 0.9233 - val_loss: 0.4823 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 156/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4865 - accuracy: 0.9231 - val_loss: 0.4817 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4865 - accuracy: 0.9231 - val_loss: 0.4817 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Epoch 157/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4863 - accuracy: 0.9229 - val_loss: 0.4828 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4863 - accuracy: 0.9229 - val_loss: 0.4828 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 158/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4861 - accuracy: 0.9230 - val_loss: 0.4837 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4861 - accuracy: 0.9230 - val_loss: 0.4837 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Epoch 159/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4857 - accuracy: 0.9231 - val_loss: 0.4839 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4857 - accuracy: 0.9231 - val_loss: 0.4839 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 160/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4851 - accuracy: 0.9232 - val_loss: 0.4821 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4851 - accuracy: 0.9232 - val_loss: 0.4821 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 161/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4854 - accuracy: 0.9233 - val_loss: 0.4841 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4854 - accuracy: 0.9233 - val_loss: 0.4841 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 162/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4852 - accuracy: 0.9234 - val_loss: 0.4813 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4852 - accuracy: 0.9234 - val_loss: 0.4813 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 163/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4848 - accuracy: 0.9235 - val_loss: 0.4833 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4848 - accuracy: 0.9235 - val_loss: 0.4833 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 164/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4848 - accuracy: 0.9235 - val_loss: 0.4817 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4848 - accuracy: 0.9235 - val_loss: 0.4817 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Epoch 165/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4851 - accuracy: 0.9231 - val_loss: 0.4826 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4851 - accuracy: 0.9231 - val_loss: 0.4826 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 166/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.9232 - val_loss: 0.4823 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4849 - accuracy: 0.9232 - val_loss: 0.4823 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Epoch 167/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4845 - accuracy: 0.9235 - val_loss: 0.4829 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4845 - accuracy: 0.9235 - val_loss: 0.4829 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Epoch 168/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4845 - accuracy: 0.9238 - val_loss: 0.4831 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4845 - accuracy: 0.9238 - val_loss: 0.4831 - val_accuracy: 0.9199 - lr: 1.0000e-04\n",
      "Epoch 169/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.9237 - val_loss: 0.4810 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.9237 - val_loss: 0.4810 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 170/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4841 - accuracy: 0.9229 - val_loss: 0.4808 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4841 - accuracy: 0.9229 - val_loss: 0.4808 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 171/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4834 - accuracy: 0.9235 - val_loss: 0.4869 - val_accuracy: 0.9179 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4834 - accuracy: 0.9235 - val_loss: 0.4869 - val_accuracy: 0.9179 - lr: 1.0000e-04\n",
      "Epoch 172/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.9232 - val_loss: 0.4827 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4837 - accuracy: 0.9232 - val_loss: 0.4827 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 173/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4837 - accuracy: 0.9233 - val_loss: 0.4839 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4837 - accuracy: 0.9233 - val_loss: 0.4839 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 174/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4838 - accuracy: 0.9230 - val_loss: 0.4801 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4838 - accuracy: 0.9230 - val_loss: 0.4801 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 175/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4831 - accuracy: 0.9238 - val_loss: 0.4803 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4831 - accuracy: 0.9238 - val_loss: 0.4803 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 176/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4831 - accuracy: 0.9236 - val_loss: 0.4786 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4831 - accuracy: 0.9236 - val_loss: 0.4786 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 177/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4828 - accuracy: 0.9238 - val_loss: 0.4830 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4828 - accuracy: 0.9238 - val_loss: 0.4830 - val_accuracy: 0.9195 - lr: 1.0000e-04\n",
      "Epoch 178/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4827 - accuracy: 0.9239 - val_loss: 0.4806 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 179/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4827 - accuracy: 0.9239 - val_loss: 0.4806 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 179/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4826 - accuracy: 0.9233 - val_loss: 0.4810 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 180/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4826 - accuracy: 0.9233 - val_loss: 0.4810 - val_accuracy: 0.9203 - lr: 1.0000e-04\n",
      "Epoch 180/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4833 - accuracy: 0.9228 - val_loss: 0.4787 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 181/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4833 - accuracy: 0.9228 - val_loss: 0.4787 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 181/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4825 - accuracy: 0.9240 - val_loss: 0.4822 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 182/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4825 - accuracy: 0.9240 - val_loss: 0.4822 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 182/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4820 - accuracy: 0.9239 - val_loss: 0.4798 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 183/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4820 - accuracy: 0.9239 - val_loss: 0.4798 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 183/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4823 - accuracy: 0.9233 - val_loss: 0.4781 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 184/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4823 - accuracy: 0.9233 - val_loss: 0.4781 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 184/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4818 - accuracy: 0.9234 - val_loss: 0.4824 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 185/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4818 - accuracy: 0.9234 - val_loss: 0.4824 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 185/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4820 - accuracy: 0.9234 - val_loss: 0.4810 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 186/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4820 - accuracy: 0.9234 - val_loss: 0.4810 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 186/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4815 - accuracy: 0.9237 - val_loss: 0.4817 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 187/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4815 - accuracy: 0.9237 - val_loss: 0.4817 - val_accuracy: 0.9209 - lr: 1.0000e-04\n",
      "Epoch 187/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4817 - accuracy: 0.9233 - val_loss: 0.4796 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 188/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4817 - accuracy: 0.9233 - val_loss: 0.4796 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 188/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.9236 - val_loss: 0.4785 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 189/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.9236 - val_loss: 0.4785 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 189/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.9238 - val_loss: 0.4790 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 190/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4813 - accuracy: 0.9238 - val_loss: 0.4790 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 190/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4811 - accuracy: 0.9238 - val_loss: 0.4783 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 191/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4811 - accuracy: 0.9238 - val_loss: 0.4783 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 191/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4810 - accuracy: 0.9237 - val_loss: 0.4814 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 192/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4810 - accuracy: 0.9237 - val_loss: 0.4814 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Epoch 192/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4809 - accuracy: 0.9231 - val_loss: 0.4775 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 193/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4809 - accuracy: 0.9231 - val_loss: 0.4775 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Epoch 193/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4805 - accuracy: 0.9241 - val_loss: 0.4779 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 194/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4805 - accuracy: 0.9241 - val_loss: 0.4779 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 194/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4802 - accuracy: 0.9245 - val_loss: 0.4787 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 195/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4802 - accuracy: 0.9245 - val_loss: 0.4787 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 195/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4804 - accuracy: 0.9236 - val_loss: 0.4786 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 196/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4804 - accuracy: 0.9236 - val_loss: 0.4786 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 196/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4800 - accuracy: 0.9239 - val_loss: 0.4765 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 197/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4800 - accuracy: 0.9239 - val_loss: 0.4765 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 197/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4800 - accuracy: 0.9240 - val_loss: 0.4810 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 198/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4800 - accuracy: 0.9240 - val_loss: 0.4810 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 198/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4799 - accuracy: 0.9245 - val_loss: 0.4790 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 199/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4799 - accuracy: 0.9245 - val_loss: 0.4790 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 199/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4795 - accuracy: 0.9235 - val_loss: 0.4774 - val_accuracy: 0.9225 - lr: 1.0000e-04\n",
      "Epoch 200/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4795 - accuracy: 0.9235 - val_loss: 0.4774 - val_accuracy: 0.9225 - lr: 1.0000e-04\n",
      "Epoch 200/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.9236 - val_loss: 0.4773 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 201/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4797 - accuracy: 0.9236 - val_loss: 0.4773 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 201/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4791 - accuracy: 0.9239 - val_loss: 0.4751 - val_accuracy: 0.9225 - lr: 1.0000e-04\n",
      "Epoch 202/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4791 - accuracy: 0.9239 - val_loss: 0.4751 - val_accuracy: 0.9225 - lr: 1.0000e-04\n",
      "Epoch 202/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4792 - accuracy: 0.9236 - val_loss: 0.4772 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 203/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4792 - accuracy: 0.9236 - val_loss: 0.4772 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 203/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4787 - accuracy: 0.9240 - val_loss: 0.4790 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 204/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4787 - accuracy: 0.9240 - val_loss: 0.4790 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 204/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4789 - accuracy: 0.9241 - val_loss: 0.4789 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 205/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4789 - accuracy: 0.9241 - val_loss: 0.4789 - val_accuracy: 0.9223 - lr: 1.0000e-04\n",
      "Epoch 205/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.9237 - val_loss: 0.4774 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 206/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4789 - accuracy: 0.9237 - val_loss: 0.4774 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 206/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4786 - accuracy: 0.9245 - val_loss: 0.4776 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 207/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4786 - accuracy: 0.9245 - val_loss: 0.4776 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 207/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4781 - accuracy: 0.9236 - val_loss: 0.4744 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 208/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4781 - accuracy: 0.9236 - val_loss: 0.4744 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 208/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4783 - accuracy: 0.9243 - val_loss: 0.4763 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 209/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4783 - accuracy: 0.9243 - val_loss: 0.4763 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 209/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.9234 - val_loss: 0.4780 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 210/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.9234 - val_loss: 0.4780 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Epoch 210/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4782 - accuracy: 0.9239 - val_loss: 0.4749 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 211/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4782 - accuracy: 0.9239 - val_loss: 0.4749 - val_accuracy: 0.9241 - lr: 1.0000e-04\n",
      "Epoch 211/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.9240 - val_loss: 0.4755 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 212/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4782 - accuracy: 0.9240 - val_loss: 0.4755 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 212/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.9248 - val_loss: 0.4801 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 213/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.9248 - val_loss: 0.4801 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Epoch 213/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4778 - accuracy: 0.9241 - val_loss: 0.4750 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 214/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4778 - accuracy: 0.9241 - val_loss: 0.4750 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Epoch 214/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4776 - accuracy: 0.9240 - val_loss: 0.4748 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 215/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4776 - accuracy: 0.9240 - val_loss: 0.4748 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 215/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.9242 - val_loss: 0.4757 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 216/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.9242 - val_loss: 0.4757 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 216/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4773 - accuracy: 0.9242 - val_loss: 0.4734 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 217/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4773 - accuracy: 0.9242 - val_loss: 0.4734 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 217/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4771 - accuracy: 0.9243 - val_loss: 0.4766 - val_accuracy: 0.9201 - lr: 1.0000e-04\n",
      "Epoch 218/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4771 - accuracy: 0.9243 - val_loss: 0.4766 - val_accuracy: 0.9201 - lr: 1.0000e-04\n",
      "Epoch 218/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.9245 - val_loss: 0.4777 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 219/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4767 - accuracy: 0.9245 - val_loss: 0.4777 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 219/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.9247 - val_loss: 0.4786 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 220/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.9247 - val_loss: 0.4786 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 220/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4769 - accuracy: 0.9239 - val_loss: 0.4748 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 221/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4769 - accuracy: 0.9239 - val_loss: 0.4748 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 221/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4766 - accuracy: 0.9238 - val_loss: 0.4727 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 222/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4766 - accuracy: 0.9238 - val_loss: 0.4727 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 222/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4759 - accuracy: 0.9250 - val_loss: 0.4738 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 223/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4759 - accuracy: 0.9250 - val_loss: 0.4738 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 223/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4763 - accuracy: 0.9242 - val_loss: 0.4744 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 224/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4763 - accuracy: 0.9242 - val_loss: 0.4744 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
      "Epoch 224/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4762 - accuracy: 0.9247 - val_loss: 0.4730 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 225/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4762 - accuracy: 0.9247 - val_loss: 0.4730 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 225/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4759 - accuracy: 0.9240 - val_loss: 0.4726 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 226/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4759 - accuracy: 0.9240 - val_loss: 0.4726 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 226/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.9247 - val_loss: 0.4735 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 227/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.9247 - val_loss: 0.4735 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 227/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.9240 - val_loss: 0.4742 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 228/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.9240 - val_loss: 0.4742 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 228/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.9245 - val_loss: 0.4737 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 229/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.9245 - val_loss: 0.4737 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 229/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.9247 - val_loss: 0.4753 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 230/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4757 - accuracy: 0.9247 - val_loss: 0.4753 - val_accuracy: 0.9213 - lr: 1.0000e-04\n",
      "Epoch 230/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.9248 - val_loss: 0.4715 - val_accuracy: 0.9233 - lr: 1.0000e-04\n",
      "Epoch 231/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.9248 - val_loss: 0.4715 - val_accuracy: 0.9233 - lr: 1.0000e-04\n",
      "Epoch 231/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4749 - accuracy: 0.9246 - val_loss: 0.4748 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 232/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4749 - accuracy: 0.9246 - val_loss: 0.4748 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 232/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4753 - accuracy: 0.9238 - val_loss: 0.4701 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 233/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4753 - accuracy: 0.9238 - val_loss: 0.4701 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 233/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4754 - accuracy: 0.9247 - val_loss: 0.4718 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 234/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4754 - accuracy: 0.9247 - val_loss: 0.4718 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 234/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4746 - accuracy: 0.9247 - val_loss: 0.4730 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Epoch 235/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4746 - accuracy: 0.9247 - val_loss: 0.4730 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Epoch 235/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4748 - accuracy: 0.9241 - val_loss: 0.4736 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 236/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4748 - accuracy: 0.9241 - val_loss: 0.4736 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 236/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4744 - accuracy: 0.9247 - val_loss: 0.4716 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 237/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4744 - accuracy: 0.9247 - val_loss: 0.4716 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 237/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4743 - accuracy: 0.9242 - val_loss: 0.4719 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 238/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4743 - accuracy: 0.9242 - val_loss: 0.4719 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 238/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4745 - accuracy: 0.9247 - val_loss: 0.4701 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 239/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4745 - accuracy: 0.9247 - val_loss: 0.4701 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 239/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4736 - accuracy: 0.9251 - val_loss: 0.4731 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 240/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4736 - accuracy: 0.9251 - val_loss: 0.4731 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 240/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4747 - accuracy: 0.9250 - val_loss: 0.4705 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 241/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4747 - accuracy: 0.9250 - val_loss: 0.4705 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 241/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4733 - accuracy: 0.9243 - val_loss: 0.4695 - val_accuracy: 0.9237 - lr: 1.0000e-04\n",
      "Epoch 242/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4733 - accuracy: 0.9243 - val_loss: 0.4695 - val_accuracy: 0.9237 - lr: 1.0000e-04\n",
      "Epoch 242/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4735 - accuracy: 0.9246 - val_loss: 0.4707 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 243/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4735 - accuracy: 0.9246 - val_loss: 0.4707 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 243/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4737 - accuracy: 0.9247 - val_loss: 0.4726 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 244/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4737 - accuracy: 0.9247 - val_loss: 0.4726 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 244/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4735 - accuracy: 0.9239 - val_loss: 0.4746 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 245/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4735 - accuracy: 0.9239 - val_loss: 0.4746 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 245/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4731 - accuracy: 0.9246 - val_loss: 0.4713 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 246/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4731 - accuracy: 0.9246 - val_loss: 0.4713 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 246/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4733 - accuracy: 0.9245 - val_loss: 0.4735 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 247/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4733 - accuracy: 0.9245 - val_loss: 0.4735 - val_accuracy: 0.9235 - lr: 1.0000e-04\n",
      "Epoch 247/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4732 - accuracy: 0.9245 - val_loss: 0.4713 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 248/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4732 - accuracy: 0.9245 - val_loss: 0.4713 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Epoch 248/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4728 - accuracy: 0.9245 - val_loss: 0.4716 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 249/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4728 - accuracy: 0.9245 - val_loss: 0.4716 - val_accuracy: 0.9229 - lr: 1.0000e-04\n",
      "Epoch 249/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4730 - accuracy: 0.9246 - val_loss: 0.4699 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 250/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4730 - accuracy: 0.9246 - val_loss: 0.4699 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Epoch 250/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4723 - accuracy: 0.9254 - val_loss: 0.4731 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 251/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4723 - accuracy: 0.9254 - val_loss: 0.4731 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 251/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4727 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 252/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4727 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 252/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4722 - accuracy: 0.9244 - val_loss: 0.4710 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Epoch 253/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4722 - accuracy: 0.9244 - val_loss: 0.4710 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Epoch 253/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4721 - accuracy: 0.9247 - val_loss: 0.4710 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 254/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4721 - accuracy: 0.9247 - val_loss: 0.4710 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 254/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4723 - accuracy: 0.9249 - val_loss: 0.4730 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 255/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4723 - accuracy: 0.9249 - val_loss: 0.4730 - val_accuracy: 0.9221 - lr: 1.0000e-04\n",
      "Epoch 255/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4724 - accuracy: 0.9251 - val_loss: 0.4706 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 256/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4724 - accuracy: 0.9251 - val_loss: 0.4706 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 256/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4719 - accuracy: 0.9245 - val_loss: 0.4697 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Epoch 257/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4719 - accuracy: 0.9245 - val_loss: 0.4697 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Epoch 257/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4716 - accuracy: 0.9249 - val_loss: 0.4706 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 258/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4716 - accuracy: 0.9249 - val_loss: 0.4706 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Epoch 258/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4716 - accuracy: 0.9246 - val_loss: 0.4712 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 259/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4716 - accuracy: 0.9246 - val_loss: 0.4712 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 259/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4713 - accuracy: 0.9251 - val_loss: 0.4699 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 260/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4713 - accuracy: 0.9251 - val_loss: 0.4699 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 260/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4714 - accuracy: 0.9248 - val_loss: 0.4700 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 261/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4714 - accuracy: 0.9248 - val_loss: 0.4700 - val_accuracy: 0.9215 - lr: 1.0000e-04\n",
      "Epoch 261/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4715 - accuracy: 0.9252 - val_loss: 0.4704 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 262/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4715 - accuracy: 0.9252 - val_loss: 0.4704 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 262/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4711 - accuracy: 0.9248 - val_loss: 0.4684 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 263/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4711 - accuracy: 0.9248 - val_loss: 0.4684 - val_accuracy: 0.9227 - lr: 1.0000e-04\n",
      "Epoch 263/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4707 - accuracy: 0.9252 - val_loss: 0.4700 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 264/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4707 - accuracy: 0.9252 - val_loss: 0.4700 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 264/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4708 - accuracy: 0.9247 - val_loss: 0.4678 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 265/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4708 - accuracy: 0.9247 - val_loss: 0.4678 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 265/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4707 - accuracy: 0.9249 - val_loss: 0.4702 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 266/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4707 - accuracy: 0.9249 - val_loss: 0.4702 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 266/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4705 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 267/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4705 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 267/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4706 - accuracy: 0.9253 - val_loss: 0.4727 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 268/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4706 - accuracy: 0.9253 - val_loss: 0.4727 - val_accuracy: 0.9211 - lr: 1.0000e-04\n",
      "Epoch 268/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4701 - accuracy: 0.9246 - val_loss: 0.4682 - val_accuracy: 0.9233 - lr: 1.0000e-04\n",
      "Epoch 269/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4701 - accuracy: 0.9246 - val_loss: 0.4682 - val_accuracy: 0.9233 - lr: 1.0000e-04\n",
      "Epoch 269/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4699 - accuracy: 0.9246 - val_loss: 0.4669 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 270/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4699 - accuracy: 0.9246 - val_loss: 0.4669 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 270/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4701 - accuracy: 0.9248 - val_loss: 0.4666 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 271/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4701 - accuracy: 0.9248 - val_loss: 0.4666 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Epoch 271/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.9249 - val_loss: 0.4696 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 272/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.9249 - val_loss: 0.4696 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 272/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4701 - accuracy: 0.9248 - val_loss: 0.4689 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 273/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4701 - accuracy: 0.9248 - val_loss: 0.4689 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 273/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.9251 - val_loss: 0.4681 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Epoch 274/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.9251 - val_loss: 0.4681 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Epoch 274/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4694 - accuracy: 0.9253 - val_loss: 0.4698 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 275/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4694 - accuracy: 0.9253 - val_loss: 0.4698 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 275/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4695 - accuracy: 0.9250 - val_loss: 0.4675 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 276/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4695 - accuracy: 0.9250 - val_loss: 0.4675 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 276/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4693 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 277/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4693 - accuracy: 0.9251 - val_loss: 0.4696 - val_accuracy: 0.9249 - lr: 1.0000e-04\n",
      "Epoch 277/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4692 - accuracy: 0.9254 - val_loss: 0.4664 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 278/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4692 - accuracy: 0.9254 - val_loss: 0.4664 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 278/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.9256 - val_loss: 0.4679 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 279/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.9256 - val_loss: 0.4679 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 279/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4686 - accuracy: 0.9252 - val_loss: 0.4684 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 280/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4686 - accuracy: 0.9252 - val_loss: 0.4684 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 280/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4682 - accuracy: 0.9255 - val_loss: 0.4695 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 281/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4682 - accuracy: 0.9255 - val_loss: 0.4695 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 281/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.9246 - val_loss: 0.4660 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 282/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.9246 - val_loss: 0.4660 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 282/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4683 - accuracy: 0.9252 - val_loss: 0.4661 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 283/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4683 - accuracy: 0.9252 - val_loss: 0.4661 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 283/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.9254 - val_loss: 0.4652 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 284/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4688 - accuracy: 0.9254 - val_loss: 0.4652 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 284/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4687 - accuracy: 0.9250 - val_loss: 0.4713 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 285/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4687 - accuracy: 0.9250 - val_loss: 0.4713 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Epoch 285/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.9253 - val_loss: 0.4657 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 286/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.9253 - val_loss: 0.4657 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 286/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4682 - accuracy: 0.9252 - val_loss: 0.4679 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 287/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4682 - accuracy: 0.9252 - val_loss: 0.4679 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 287/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4685 - accuracy: 0.9249 - val_loss: 0.4674 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 288/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4685 - accuracy: 0.9249 - val_loss: 0.4674 - val_accuracy: 0.9231 - lr: 1.0000e-04\n",
      "Epoch 288/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.9256 - val_loss: 0.4665 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 289/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4680 - accuracy: 0.9256 - val_loss: 0.4665 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 289/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4674 - accuracy: 0.9254 - val_loss: 0.4658 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 290/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4674 - accuracy: 0.9254 - val_loss: 0.4658 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 290/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.9254 - val_loss: 0.4641 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 291/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.9254 - val_loss: 0.4641 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 291/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.9252 - val_loss: 0.4679 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 292/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.9252 - val_loss: 0.4679 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 292/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9254 - val_loss: 0.4706 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 293/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9254 - val_loss: 0.4706 - val_accuracy: 0.9219 - lr: 1.0000e-04\n",
      "Epoch 293/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4674 - accuracy: 0.9252 - val_loss: 0.4642 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 294/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4674 - accuracy: 0.9252 - val_loss: 0.4642 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Epoch 294/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9252 - val_loss: 0.4649 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 295/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9252 - val_loss: 0.4649 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Epoch 295/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4665 - accuracy: 0.9258 - val_loss: 0.4672 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 296/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4665 - accuracy: 0.9258 - val_loss: 0.4672 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Epoch 296/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4664 - accuracy: 0.9259 - val_loss: 0.4644 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 297/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4664 - accuracy: 0.9259 - val_loss: 0.4644 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 297/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9251 - val_loss: 0.4648 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 298/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4671 - accuracy: 0.9251 - val_loss: 0.4648 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Epoch 298/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4666 - accuracy: 0.9256 - val_loss: 0.4629 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 299/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4666 - accuracy: 0.9256 - val_loss: 0.4629 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Epoch 299/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4666 - accuracy: 0.9250 - val_loss: 0.4647 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 300/300\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4666 - accuracy: 0.9250 - val_loss: 0.4647 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 300/300\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4660 - accuracy: 0.9251 - val_loss: 0.4649 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "\n",
      "Model training completed.\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.4660 - accuracy: 0.9251 - val_loss: 0.4649 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "\n",
      "Model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train with more epochs and data augmentation\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=300,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nModel training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffc97f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model on test data...\n",
      "Test Loss: 0.4649\n",
      "Test Accuracy: 0.9254\n",
      "\n",
      "Making predictions on a few test images...\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000190E967BF60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Test Loss: 0.4649\n",
      "Test Accuracy: 0.9254\n",
      "\n",
      "Making predictions on a few test images...\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000190E967BF60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000190E967BF60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x2500 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference test completed. Predicted vs. True labels for 25 samples displayed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict a few test images\n",
    "print(\"\\nMaking predictions on a few test images...\")\n",
    "predictions = model.predict(x_test[:25]) # Predict on the first 5 test images\n",
    "\n",
    "# Display images and predictions\n",
    "plt.figure(figsize=(25, 25))\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {np.argmax(y_test[i])}\\nPred: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInference test completed. Predicted vs. True labels for 25 samples displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "08091bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Analysis for SiFive FE310-G002 Board:\n",
      "Available Memory:\n",
      "- Instruction Cache (L1): 16KB\n",
      "- Data SRAM (L1): 16KB\n",
      "- ROM: 8KB\n",
      "- On-board SPI Flash: 32 Mbit\n",
      "\n",
      "Model Memory Requirements:\n",
      "Total Model Parameters: 25818\n",
      "Estimated Memory for Weights (int4): 12909 bytes (12.61 KB)\n",
      "Estimated Activation Memory: 1586 bytes (1.55 KB)\n",
      "Total Memory Requirement: 14495 bytes (14.16 KB)\n",
      "\n",
      "Data SRAM Utilization: 88.5% of 16KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Calculate model memory footprint for SiFive FE310-G002\n",
    "print(\"\\nMemory Analysis for SiFive FE310-G002 Board:\")\n",
    "print(\"Available Memory:\")\n",
    "print(\"- Instruction Cache (L1): 16KB\")\n",
    "print(\"- Data SRAM (L1): 16KB\")\n",
    "print(\"- ROM: 8KB\")\n",
    "print(\"- On-board SPI Flash: 32 Mbit\")\n",
    "\n",
    "# Calculate model parameters memory (now using int4 = 0.5 bytes per parameter)\n",
    "total_params = model.count_params()\n",
    "weights_memory_bytes = (total_params * 0.5)  # 4 bits per parameter\n",
    "weights_memory_kb = weights_memory_bytes / 1024\n",
    "\n",
    "print(f\"\\nModel Memory Requirements:\")\n",
    "print(f\"Total Model Parameters: {total_params}\")\n",
    "print(f\"Estimated Memory for Weights (int4): {weights_memory_bytes:.0f} bytes ({weights_memory_kb:.2f} KB)\")\n",
    "\n",
    "# Calculate activation memory for inference\n",
    "input_mem = 28 * 28 * 1  # Input layer (int8)\n",
    "flatten_output_mem = 784  # Flattened input (int8)\n",
    "dense1_output_mem = 8    # Hidden layer output (int8)\n",
    "dense2_output_mem = 10   # Output layer (int8)\n",
    "\n",
    "total_activation_mem = input_mem + flatten_output_mem + dense1_output_mem + dense2_output_mem\n",
    "print(f\"Estimated Activation Memory: {total_activation_mem} bytes ({total_activation_mem/1024:.2f} KB)\")\n",
    "\n",
    "# Total memory requirement\n",
    "total_memory = weights_memory_bytes + total_activation_mem\n",
    "print(f\"Total Memory Requirement: {total_memory:.0f} bytes ({total_memory/1024:.2f} KB)\")\n",
    "\n",
    "# Memory utilization analysis\n",
    "data_sram_utilization = (total_memory / (16 * 1024)) * 100\n",
    "print(f\"\\nData SRAM Utilization: {data_sram_utilization:.1f}% of 16KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15769287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keras model saved to mnist_baseline_model.keras\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmp1vw0klu7\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmp1vw0klu7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\shwet\\AppData\\Local\\Temp\\tmp1vw0klu7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Int4 quantized TFLite model saved to mnist_quantized_model.tflite\n",
      "Int4 Quantized Model Size: 28888 bytes (28.21 KB)\n",
      "Cleaned up temporary Keras model file: mnist_baseline_model.keras\n",
      "\n",
      "Model Details:\n",
      "Input: [{'name': 'serving_default_flatten_5_input:0', 'index': 0, 'shape': array([ 1, 28, 28,  1]), 'shape_signature': array([-1, 28, 28,  1]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.003921568859368563, -128), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output: [{'name': 'StatefulPartitionedCall:0', 'index': 12, 'shape': array([ 1, 10]), 'shape_signature': array([-1, 10]), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128]), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "Quantization Parameters:\n",
      "Input Scale: [0.00392157]\n",
      "Input Zero Point: [-128]\n",
      "Output Scale: [0.00390625]\n",
      "Output Zero Point: [-128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shwet\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Save the model in Keras format\n",
    "keras_model_path = \"mnist_baseline_model.keras\"\n",
    "model.save(keras_model_path)\n",
    "print(f\"\\nKeras model saved to {keras_model_path}\")\n",
    "\n",
    "# Load the model for conversion\n",
    "loaded_model = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "# Create TFLite converteconverter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "# Create TFLite converter with INT4 quantization\n",
    "# Enhanced quantization configuration\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Set target spec for INT4\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "]\n",
    "\n",
    "# Increase calibration samples\n",
    "def representative_data_gen():\n",
    "    num_calibration_samples = 1000  # Increased from 100\n",
    "    for i in range(num_calibration_samples):\n",
    "        # Add noise for robustness\n",
    "        image = x_train[i:i+1].astype(np.float32)\n",
    "        image += np.random.normal(0, 0.01, image.shape)\n",
    "        image = np.clip(image, 0, 1)\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "\n",
    "# Force 4-bit weights through post-training quantization\n",
    "converter.target_spec._experimental_low_bit_qat = True\n",
    "converter.target_spec._experimental_weight_bits = 4\n",
    "\n",
    "# Convert model to TFLite format\n",
    "tflite_model_quant = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "with open(tflite_model_quant_filename, \"wb\") as f:\n",
    "    f.write(tflite_model_quant)\n",
    "\n",
    "print(f\"\\nInt4 quantized TFLite model saved to {tflite_model_quant_filename}\")\n",
    "\n",
    "# Check model size\n",
    "tflite_model_quant_size_bytes = os.path.getsize(tflite_model_quant_filename)\n",
    "tflite_model_quant_size_kb = tflite_model_quant_size_bytes / 1024\n",
    "print(f\"Int4 Quantized Model Size: {tflite_model_quant_size_bytes} bytes ({tflite_model_quant_size_kb:.2f} KB)\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(keras_model_path)\n",
    "print(f\"Cleaned up temporary Keras model file: {keras_model_path}\")\n",
    "\n",
    "# Create an interpreter to analyze the model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_quant_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"\\nModel Details:\")\n",
    "print(\"Input:\", input_details)\n",
    "print(\"Output:\", output_details)\n",
    "\n",
    "# Print quantization parameters\n",
    "print(\"\\nQuantization Parameters:\")\n",
    "print(f\"Input Scale: {input_details[0]['quantization_parameters']['scales']}\")\n",
    "print(f\"Input Zero Point: {input_details[0]['quantization_parameters']['zero_points']}\")\n",
    "print(f\"Output Scale: {output_details[0]['quantization_parameters']['scales']}\")\n",
    "print(f\"Output Zero Point: {output_details[0]['quantization_parameters']['zero_points']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1aa5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C header file generated: mnist_model_data.h\n",
      "This file contains the quantized model as a byte array, ready for embedded C inclusion.\n"
     ]
    }
   ],
   "source": [
    "# Generate a .h file for the quantized TFLite model\n",
    "\n",
    "import os\n",
    "\n",
    "tflite_model_quant_filename = \"mnist_quantized_model.tflite\"\n",
    "c_model_filename = \"mnist_model_data.h\"\n",
    "\n",
    "# Read the TFLite model file as bytes\n",
    "with open(tflite_model_quant_filename, \"rb\") as f:\n",
    "    tflite_model_bytes = f.read()\n",
    "\n",
    "# Convert bytes to a C-style array declaration\n",
    "# Format: const unsigned char model_name[] = { 0x.., 0x.., ... };\n",
    "bytes_as_hex = [f\"0x{byte:02x}\" for byte in tflite_model_bytes]\n",
    "c_array_declaration = f\"const unsigned char {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_data[] = {{\\n    \"\n",
    "c_array_declaration += \", \".join(bytes_as_hex)\n",
    "c_array_declaration += \"\\n};\\n\"\n",
    "c_array_declaration += f\"const int {os.path.basename(tflite_model_quant_filename).replace('.', '_')}_len = {len(tflite_model_bytes)};\\n\"\n",
    "\n",
    "# Write the C array declaration to the .h file\n",
    "with open(c_model_filename, \"w\") as f:\n",
    "    f.write(\"#ifndef MNIST_MODEL_DATA_H_\\n\")\n",
    "    f.write(\"#define MNIST_MODEL_DATA_H_\\n\\n\")\n",
    "    f.write(c_array_declaration)\n",
    "    f.write(\"\\n#endif // MNIST_MODEL_DATA_H_\\n\")\n",
    "\n",
    "print(f\"\\nC header file generated: {c_model_filename}\")\n",
    "print(f\"This file contains the quantized model as a byte array, ready for embedded C inclusion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
