# generate_c_model_params.py
import tensorflow as tf
import numpy as np
import os

# --- Configuration ---
TFLITE_MODEL_PATH = "src/mnist_quantized_model.tflite"
C_HEADER_FILE = "src/mnist_model_params.h"
C_SOURCE_FILE = "src/mnist_model_params.c"

# --- Helper function for quantization of input data ---
def quantize_input(image_data_float32, scale, zero_point):
    quantized_data = np.round(image_data_float32 / scale + zero_point)
    quantized_data = np.clip(quantized_data, -128, 127).astype(np.int8)
    return quantized_data

# --- Main generation function ---
def generate_c_arrays_from_tflite(model_path, header_file, source_file):
    print(f"Loading TFLite model from: {model_path}")
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    tensor_details = interpreter.get_tensor_details()
    ops_details_raw = interpreter._get_ops_details()

    with open(header_file, "w") as hf, open(source_file, "w") as sf:
        hf.write("/*\n")
        hf.write(" * This file is auto-generated by generate_c_model_params.py.\n")
        hf.write(" * Do not modify manually.\n")
        hf.write(" */\n")
        hf.write("#ifndef MNIST_MODEL_PARAMS_H\n")
        hf.write("#define MNIST_MODEL_PARAMS_H\n\n")
        hf.write("#include <stdint.h>\n\n")
        
        sf.write("/*\n")
        sf.write(" * This file is auto-generated by generate_c_model_params.py.\n")
        sf.write(" * Do not modify manually.\n")
        sf.write(" */\n")
        sf.write("#include \"mnist_model_params.h\"\n\n")
        
        hf.write("#define MAX_N_ACTIVATIONS 64\n")
        hf.write("#define INPUT_SCALE_FACTOR 64\n")
        hf.write("#define ACTIVATION_BITS 8\n")
        hf.write("#define BATCH_SIZE 16\n\n")
        
        hf.write("// Add buffer overflow protection\n")
        hf.write("#define SAFE_ACCESS(arr, idx, max) ((idx) < (max) ? arr[idx] : 0)\n\n")
        hf.write("// Add memory alignment for better performance\n")
        hf.write("#define ALIGN_16 __attribute__((aligned(16)))\n\n")

        layer_idx = 1
        for op_detail in ops_details_raw:
            current_op_name = op_detail.get('op_name')
            current_builtin_code = op_detail.get('builtin_code')

            is_fully_connected = False
            if current_op_name == 'FULLY_CONNECTED':
                is_fully_connected = True
            elif current_builtin_code == 9:
                is_fully_connected = True

            if is_fully_connected:
                print(f"Processing FULLY_CONNECTED layer {layer_idx}...")
                input_tensor_idx = op_detail['inputs'][0]
                weights_tensor_idx = op_detail['inputs'][1]
                biases_tensor_idx = op_detail['inputs'][2] if len(op_detail['inputs']) > 2 else -1
                output_tensor_idx = op_detail['outputs'][0]

                input_tensor = next(t for t in tensor_details if t['index'] == input_tensor_idx)
                weights_tensor = next(t for t in tensor_details if t['index'] == weights_tensor_idx)
                output_tensor = next(t for t in tensor_details if t['index'] == output_tensor_idx)
                
                weights_data = interpreter.tensor(weights_tensor_idx)()
                biases_data = interpreter.tensor(biases_tensor_idx)() if biases_tensor_idx != -1 else None

                input_scale = input_tensor['quantization_parameters'].get('scales', [1.0])[0]
                input_zero_point = input_tensor['quantization_parameters'].get('zero_points', [0])[0]
                output_scale = output_tensor['quantization_parameters'].get('scales', [1.0])[0]
                output_zero_point = output_tensor['quantization_parameters'].get('zero_points', [0])[0]
                weights_scale = weights_tensor['quantization_parameters'].get('scales', [1.0])[0]
                weights_zero_point = weights_tensor['quantization_parameters'].get('zero_points', [0])[0]

                incoming_weights = weights_tensor['shape'][1]
                outgoing_weights = weights_tensor['shape'][0]

                packed_weights = []
                flat_weights = weights_data.flatten()
                if len(flat_weights) % 4 != 0:
                    flat_weights = np.pad(flat_weights, (0, 4 - (len(flat_weights) % 4)), 'constant', constant_values=0)

                for i in range(0, len(flat_weights), 4):
                    val = 0
                    for j in range(4):
                        val |= (np.uint32(flat_weights[i+j].astype(np.uint8)) << (24 - j*8))
                    packed_weights.append(f"0x{val:08X}")

                biases_hex = [f"{b}" for b in biases_data] if biases_data is not None else []
                
                hf.write(f"// Layer {layer_idx} Parameters\n")
                hf.write(f"extern const uint32_t ALIGN_16 L{layer_idx}_weights[{len(packed_weights)}];\n")
                if biases_data is not None:
                    hf.write(f"extern const int32_t ALIGN_16 L{layer_idx}_biases[{len(biases_hex)}];\n")
                hf.write(f"extern const int32_t L{layer_idx}_bitperweight;\n")
                hf.write(f"extern const uint32_t L{layer_idx}_incoming_weights;\n")
                hf.write(f"extern const uint32_t L{layer_idx}_outgoing_weights;\n")
                hf.write(f"extern const float L{layer_idx}_input_scale;\n")
                hf.write(f"extern const int32_t L{layer_idx}_input_zero_point;\n")
                hf.write(f"extern const float L{layer_idx}_output_scale;\n")
                hf.write(f"extern const int32_t L{layer_idx}_output_zero_point;\n")
                hf.write(f"extern const float L{layer_idx}_weights_scale;\n")
                hf.write(f"extern const int32_t L{layer_idx}_weights_zero_point;\n\n")

                sf.write(f"// Layer {layer_idx} Parameters\n")
                sf.write(f"const uint32_t ALIGN_16 L{layer_idx}_weights[{len(packed_weights)}] = {{\n    {', '.join(packed_weights)}\n}};\n")
                if biases_data is not None:
                    sf.write(f"const int32_t ALIGN_16 L{layer_idx}_biases[{len(biases_hex)}] = {{\n    {', '.join(biases_hex)}\n}};\n")
                sf.write(f"const int32_t L{layer_idx}_bitperweight = 8;\n")
                sf.write(f"const uint32_t L{layer_idx}_incoming_weights = {incoming_weights};\n")
                sf.write(f"const uint32_t L{layer_idx}_outgoing_weights = {outgoing_weights};\n")
                sf.write(f"const float L{layer_idx}_input_scale = {input_scale:.8f}f;\n")
                sf.write(f"const int32_t L{layer_idx}_input_zero_point = {input_zero_point};\n")
                sf.write(f"const float L{layer_idx}_output_scale = {output_scale:.8f}f;\n")
                sf.write(f"const int32_t L{layer_idx}_output_zero_point = {output_zero_point};\n")
                sf.write(f"const float L{layer_idx}_weights_scale = {weights_scale:.8f}f;\n")
                sf.write(f"const int32_t L{layer_idx}_weights_zero_point = {weights_zero_point};\n\n")

                layer_idx += 1

        print("Generating quantized sample inputs and labels...")
        (_, _), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data()
        x_test_processed = x_test_raw.reshape(-1, 28, 28, 1).astype("float32") / 255.0

        model_input_details = interpreter.get_input_details()[0]
        model_input_scale = model_input_details['quantization_parameters'].get('scales', [1.0])[0]
        model_input_zero_point = model_input_details['quantization_parameters'].get('zero_points', [0])[0]

        num_samples_to_generate = 10
        hf.write("// Quantized sample input images and their labels\n")
        sf.write("// Quantized sample input images and their labels\n")

        for i in range(num_samples_to_generate):
            original_image_float = x_test_processed[i].flatten()
            quantized_image_int8 = quantize_input(original_image_float, model_input_scale, model_input_zero_point)
            label = y_test_raw[i]
            image_c_array_content = ', '.join([f"{val}" for val in quantized_image_int8])
            hf.write(f"extern const int8_t input_data_{i}[{len(quantized_image_int8)}];\n")
            hf.write(f"extern const uint8_t label_{i};\n")
            sf.write(f"const int8_t input_data_{i}[{len(quantized_image_int8)}] = {{\n    {image_c_array_content}\n}};\n")
            sf.write(f"const uint8_t label_{i} = {label};\n")

        hf.write("\n#endif // MNIST_MODEL_PARAMS_H\n")
    print(f"Generated {header_file} and {source_file} with model parameters and sample inputs.")

if __name__ == "__main__":
    if not os.path.exists(TFLITE_MODEL_PATH):
        print(f"Error: Quantized TFLite model not found at {TFLITE_MODEL_PATH}.")
        print("Please ensure you have run the `mnist_baseline_model.ipynb` notebook")
        print("to generate `mnist_quantized_model.tflite` before running this script.")
    else:
        generate_c_arrays_from_tflite(TFLITE_MODEL_PATH, C_HEADER_FILE, C_SOURCE_FILE)